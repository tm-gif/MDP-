{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib notebook\n",
    "from dateutil.parser import parse \n",
    "import nltk \n",
    "from rapidfuzz import process, fuzz #Levenshtein Distance Calculation\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import random as rm\n",
    "\n",
    "# import tkinter as tk #desktop based, does not work with web based jupyter \n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3636363636363598\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "f = open('Job_Desc/JobDescr2.txt', 'r')\n",
    "\n",
    "data = f.read()\n",
    "split_job = data.split()\n",
    "split_job[:] = (value for value in split_job if value != '\\t')\n",
    "   \n",
    "d = open('Resumes/ResumeSample3.txt', 'r')\n",
    "\n",
    "data = d.read()\n",
    "split_resume = data.split()\n",
    "split_resume[:] = (value for value in split_resume if value != '\\t')\n",
    "\n",
    "text_tokens_resume = split_resume\n",
    "text_tokens_job = split_job\n",
    "\n",
    "tokens_without_sw1 = [word for word in text_tokens_resume if not word in stopwords.words()]\n",
    "tokens_without_sw = [word for word in text_tokens_job if not word in stopwords.words()]\n",
    "\n",
    "\n",
    "\n",
    "#Calculate the Levenshtein distance\n",
    "\"\"\"\n",
    "    The Levenshtein distance is a string metric for measuring the difference\n",
    "    between two sequences.\n",
    "    It is calculated as the minimum number of single-character edits necessary to\n",
    "    transform one string into another.\n",
    "    \"\"\"\n",
    "#fuzz.ratio(tokens_without_sw1, tokens_without_sw)\n",
    "#fuzz.token_set_ratio(tokens_without_sw1, tokens_without_sw)\n",
    "\n",
    "Levenshtein = fuzz.token_set_ratio(tokens_without_sw1, tokens_without_sw)\n",
    "Lev = Levenshtein\n",
    "print(Levenshtein)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning with mdp_game(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------episode-------------------------- 0\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 1\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 2\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 3\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 4\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 5\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 6\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 7\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "EXPLOITING ACTION PERFORMED 1\n",
      "In State Skills\n",
      "reward -1.0\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "EXPLOITING\n",
      "EXPLOITING ACTION PERFORMED 2\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "EXPLOITING ACTION PERFORMED 0\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 8\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "EXPLOITING ACTION PERFORMED 2\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 9\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 10\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 11\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 12\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 13\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 14\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 15\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 16\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 17\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 18\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 19\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 20\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 21\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 22\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 23\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 24\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 25\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 26\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 27\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 28\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 29\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 30\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 31\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 32\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 33\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 34\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 35\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 36\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 37\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 38\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 39\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 40\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 41\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 42\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 43\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 44\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 45\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 46\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 47\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 48\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 49\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 50\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 51\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 52\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 53\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 54\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 55\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 56\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 57\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 58\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 59\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 60\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 61\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 62\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 63\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 64\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 65\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 66\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 67\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 68\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 69\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 70\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 71\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 72\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 73\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 74\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 75\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 76\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 77\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 78\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 79\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 80\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 81\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 82\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 83\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 84\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 85\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLORING\n",
      "EXPLORING ACTION PERFORMED 1\n",
      "next_state1 1\n",
      "In State Skills\n",
      "reward -1.0\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "EXPLORING\n",
      "EXPLORING ACTION PERFORMED 0\n",
      "next_state1 0\n",
      "In State Education\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 86\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 87\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 88\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 89\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 90\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 91\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 92\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 93\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 94\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 95\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 96\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [10. -1. -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 10.0\n",
      "--------------------------episode-------------------------- 97\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 98\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 99\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "[[ 0.       -0.2      24.987324]\n",
      " [20.461315  0.        1.      ]\n",
      " [29.989859  0.        0.      ]\n",
      " [ 0.        0.        0.      ]\n",
      " [ 0.        0.        0.      ]]\n",
      "Greedy traversal for starting state 0\n",
      "0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0\n",
      "\n",
      "Greedy traversal for starting state 1\n",
      "1 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n",
      "Greedy traversal for starting state 2\n",
      "2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n",
      "Greedy traversal for starting state 3\n",
      "3 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n",
      "Greedy traversal for starting state 4\n",
      "4 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                r -= 1\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r += .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "\n",
    "\n",
    "# defines the reward/connection graph\n",
    "r = np.array([[.25, -1,  1],\n",
    "              [ .5, .5, 1],\n",
    "              [ 10, -1,  -1],\n",
    "              [-1,  -1,  1],\n",
    "              [ 0, -1, 1]]).astype(\"float32\")\n",
    "\n",
    "\n",
    "q = np.zeros_like(r)\n",
    "\n",
    "gamma = 0.8\n",
    "alpha = 1.\n",
    "n_episodes = 100\n",
    "n_states = 5\n",
    "n_actions = 4\n",
    "epsilon = 0.05\n",
    "random_state = np.random.RandomState(1999)\n",
    "\n",
    "state_grid = [[0 for i in range(n_states)] for i in range(n_actions)] #2States x 3 Actions\n",
    "reward_grid = [[0 for i in range(n_states)] for i in range(n_actions)] #2States x 3 Actions\n",
    "\n",
    "\n",
    "def state_space(s):\n",
    "    if(s == 0):\n",
    "        print(\"In State Education\")\n",
    "    elif(s == 1):\n",
    "        print(\"In State Skills\")\n",
    "    elif(s == 2):\n",
    "        print(\"In State Experience\")\n",
    "    elif(s == 3):\n",
    "        print(\"In State Certifications\")\n",
    "    elif(s == 4):\n",
    "        print(\"In State Referrals\")        \n",
    "    return\n",
    "\n",
    "\n",
    "def action_space(a):\n",
    "    if(a == 0):\n",
    "        print(\"Action: Accept Qualifications\")\n",
    "    elif(a == 1):\n",
    "        print(\"Action: Decline Qualifications\")\n",
    "    elif(a == 2):\n",
    "        print(\"Action: Review Qualifications\")\n",
    "    elif(a == 3):\n",
    "        print(\"Action: Examine Other Candidates\")\n",
    "    return\n",
    "\n",
    "\n",
    "def show_traverse():\n",
    "    # show all the greedy traversals\n",
    "    for i in range(len(q)):\n",
    "        curr_state = i\n",
    "        traverse = \"%i -> \" % curr_state\n",
    "        n_steps = 0\n",
    "        while curr_state != 5 and n_steps < 20:\n",
    "            new_state = np.argmax(q[curr_state])\n",
    "            curr_state = new_state\n",
    "            traverse += \"%i -> \" % curr_state\n",
    "            n_steps = n_steps + 1\n",
    "        # cut off final arrow\n",
    "        traverse = traverse[:-4]\n",
    "        print(\"Greedy traversal for starting state %i\" % i)\n",
    "        print(traverse)\n",
    "        print(\"\")\n",
    "        \n",
    "        \n",
    "def update_q(state, new_state, action):\n",
    "    \n",
    "    q[state, action] = q[state, action] + alpha * (r[state, action] + gamma * max(q[new_state, :]) - q[state, action])\n",
    "    return r[state, action]\n",
    "\n",
    "\n",
    "def mdp():\n",
    "    for e in range(int(n_episodes)):\n",
    "        print(\"--------------------------episode--------------------------\", e)\n",
    "\n",
    "        states = list(range(n_states))\n",
    "\n",
    "        print(\"states\", states)\n",
    "\n",
    "        random_state.shuffle(states) #shuffles order of states\n",
    "        curr_state = states[0] #assigns current state as a random state\n",
    "\n",
    "        goal = False\n",
    "\n",
    "        print(\"current_state\", curr_state)    \n",
    "        if (curr_state == 0):  \n",
    "            print(\"STATE 1\")\n",
    "\n",
    "            while not goal: \n",
    "                # epsilon greedy\n",
    "                valid_moves = r[curr_state] >= 0\n",
    "                print(\"r[current_state]\", r[curr_state])\n",
    "\n",
    "                if rm.uniform(0, 1) < epsilon: #Explore\n",
    "                    print(\"EXPLORING\")\n",
    "                    actions = np.array(list(range(n_actions-1)))\n",
    "                    random_state.shuffle(actions)\n",
    "                    action = actions[0]\n",
    "                    print(\"EXPLORING ACTION PERFORMED\", action)\n",
    "                    new_state = action #choose random action\n",
    "\n",
    "                    print(\"next_state1\", new_state)\n",
    "                    state_space(new_state)\n",
    "\n",
    "                else: #Exploit Q table\n",
    "                    print(\"EXPLOITING\")\n",
    "                    if np.sum(q[curr_state]) > 0:\n",
    "                        action = np.argmax(q[curr_state]) #action is the max of the q-val in the current state\n",
    "                    else:\n",
    "                        # Don't allow invalid moves at the start\n",
    "                        # Just take a random move\n",
    "                        actions = np.array(list(range(n_actions-1)))\n",
    "                        random_state.shuffle(actions)\n",
    "                        action = actions[0]\n",
    "                        print(\"EXPLOITING ACTION PERFORMED\", action)\n",
    "\n",
    "\n",
    "                    new_state = action\n",
    "                    state_space(new_state)\n",
    "\n",
    "                reward = update_q(curr_state, new_state, action)\n",
    "                print(\"reward\", reward)\n",
    "                \n",
    "                # Goal state has reward 100\n",
    "                if reward > 1:\n",
    "                    goal = True\n",
    "                curr_state = new_state\n",
    "\n",
    "   \n",
    "    print(q)\n",
    "    show_traverse()\n",
    "    #show_q()\n",
    "    \n",
    "mdp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOST FUNCTIONS=ING Q-LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------episode-------------------------- 0\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 1\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 2\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 3\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 4\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 5\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 6\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 7\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "EXPLOITING ACTION PERFORMED 2\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLORING\n",
      "CHECKING VALID ACTIONS [0]\n",
      "EXPLORING ACTION PERFORMED 0\n",
      "next_state1 0\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 8\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 9\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLORING\n",
      "CHECKING VALID ACTIONS [0 2]\n",
      "EXPLORING ACTION PERFORMED 0\n",
      "next_state1 0\n",
      "In State Education\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLORING\n",
      "CHECKING VALID ACTIONS [0 2]\n",
      "EXPLORING ACTION PERFORMED 2\n",
      "next_state1 2\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 10\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 11\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 12\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 13\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 14\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 15\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 16\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 17\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 18\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 19\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 20\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 21\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 22\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 23\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 24\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLORING\n",
      "CHECKING VALID ACTIONS [0 2]\n",
      "EXPLORING ACTION PERFORMED 2\n",
      "next_state1 2\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 25\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 26\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 27\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 28\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 29\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 30\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 31\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 32\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 33\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 34\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 35\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 36\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 37\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 38\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 39\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 40\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 41\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 42\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 43\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 44\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 45\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 46\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 47\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 48\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 49\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 50\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 51\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 52\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 53\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 54\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 55\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 56\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 57\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 58\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 59\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 60\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 61\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 62\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 63\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLORING\n",
      "CHECKING VALID ACTIONS [0]\n",
      "EXPLORING ACTION PERFORMED 0\n",
      "next_state1 0\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 64\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 65\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 66\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 67\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 68\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 69\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 70\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 71\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 72\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 73\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 74\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 75\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 76\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 77\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 78\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 79\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 80\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 81\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 82\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 83\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 84\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 85\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 86\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 87\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 88\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 89\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 90\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "--------------------------episode-------------------------- 91\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 92\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 3\n",
      "--------------------------episode-------------------------- 93\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 94\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 95\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 1\n",
      "--------------------------episode-------------------------- 96\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 97\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "STATE 1\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "EXPLOITING\n",
      "In State Experience\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "EXPLOITING\n",
      "In State Education\n",
      "reward 100.0\n",
      "--------------------------episode-------------------------- 98\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 2\n",
      "--------------------------episode-------------------------- 99\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 4\n",
      "[[  1.1975712   0.        224.88643  ]\n",
      " [  0.          0.          0.       ]\n",
      " [279.90915     0.          0.       ]\n",
      " [  0.          0.          0.       ]\n",
      " [  0.          0.          0.       ]]\n",
      "Greedy traversal for starting state 0\n",
      "0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0\n",
      "\n",
      "Greedy traversal for starting state 1\n",
      "1 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n",
      "Greedy traversal for starting state 2\n",
      "2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n",
      "Greedy traversal for starting state 3\n",
      "3 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n",
      "Greedy traversal for starting state 4\n",
      "4 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# defines the reward/connection graph\n",
    "r = np.array([[.25, -1,  1],\n",
    "              [ .5, .5, 1],\n",
    "              [ 100, -1,  -1],\n",
    "              [-1,  -1,  1],\n",
    "              [ 0, -1, 100]]).astype(\"float32\")\n",
    "\n",
    "\n",
    "q = np.zeros_like(r)\n",
    "\n",
    "gamma = 0.8\n",
    "alpha = 1.\n",
    "n_episodes = 100\n",
    "n_states = 5\n",
    "n_actions = 4\n",
    "epsilon = 0.05\n",
    "random_state = np.random.RandomState(1999)\n",
    "\n",
    "state_grid = [[0 for i in range(n_states)] for i in range(n_actions)] #2States x 3 Actions\n",
    "reward_grid = [[0 for i in range(n_states)] for i in range(n_actions)] #2States x 3 Actions\n",
    "\n",
    "\n",
    "def state_space(s):\n",
    "    if(s == 0):\n",
    "        print(\"In State Education\")\n",
    "    elif(s == 1):\n",
    "        print(\"In State Skills\")\n",
    "    elif(s == 2):\n",
    "        print(\"In State Experience\")\n",
    "    elif(s == 3):\n",
    "        print(\"In State Certifications\")\n",
    "    elif(s == 4):\n",
    "        print(\"In State Referrals\")        \n",
    "    return\n",
    "\n",
    "\n",
    "def action_space(a):\n",
    "    if(a == 0):\n",
    "        print(\"Action: Accept Qualifications\")\n",
    "    elif(a == 1):\n",
    "        print(\"Action: Decline Qualifications\")\n",
    "    elif(a == 2):\n",
    "        print(\"Action: Review Qualifications\")\n",
    "    elif(a == 3):\n",
    "        print(\"Action: Examine Other Candidates\")\n",
    "    return\n",
    "\n",
    "\n",
    "def show_traverse():\n",
    "    # show all the greedy traversals\n",
    "    for i in range(len(q)):\n",
    "        current_state = i\n",
    "        traverse = \"%i -> \" % current_state\n",
    "        n_steps = 0\n",
    "        while current_state != 5 and n_steps < 20:\n",
    "            next_state = np.argmax(q[current_state])\n",
    "            current_state = next_state\n",
    "            traverse += \"%i -> \" % current_state\n",
    "            n_steps = n_steps + 1\n",
    "        # cut off final arrow\n",
    "        traverse = traverse[:-4]\n",
    "        print(\"Greedy traversal for starting state %i\" % i)\n",
    "        print(traverse)\n",
    "        print(\"\")\n",
    "        \n",
    "        \n",
    "def update_q(state, next_state, action):\n",
    "    \n",
    "    q[state, action] = q[state, action] + alpha * (r[state, action] + gamma * max(q[next_state, :]) - q[state, action])\n",
    "    # renormalize row to be between 0 and 1\n",
    "#     rn = q[state][q[state] > 0] / np.sum(q[state][q[state] > 0])\n",
    "#     q[state][q[state] > 0] = rn\n",
    "    return r[state, action]\n",
    "\n",
    "\n",
    "def mdp():\n",
    "    for e in range(int(n_episodes)):\n",
    "        print(\"--------------------------episode--------------------------\", e)\n",
    "\n",
    "        states = list(range(n_states))\n",
    "\n",
    "        print(\"states\", states)\n",
    "\n",
    "        random_state.shuffle(states) #shuffles order of states\n",
    "        current_state = states[0] #assigns current state as a random state\n",
    "\n",
    "        goal = False\n",
    "    #     if e % int(n_episodes / 10.) == 0 and e > 0:\n",
    "    #         pass\n",
    "\n",
    "            # uncomment this to see plots each monitoring\n",
    "            #show_traverse()\n",
    "            #show_q()\n",
    "        print(\"current_state\", current_state)    \n",
    "        if (current_state == 0):  \n",
    "            print(\"STATE 1\")\n",
    "\n",
    "            while not goal: \n",
    "                # epsilon greedy\n",
    "                valid_moves = r[current_state] >= 0\n",
    "    #             print(\"current_state\", current_state)\n",
    "                print(\"r[current_state]\", r[current_state])\n",
    "\n",
    "                if rm.uniform(0, 1) < epsilon: #Explore\n",
    "                    print(\"EXPLORING\")\n",
    "                    actions = np.array(list(range(n_actions-1)))\n",
    "                    actions = actions[valid_moves == True]\n",
    "                    print(\"CHECKING VALID ACTIONS\", actions)\n",
    "                    if type(actions) is int:\n",
    "                        actions = [actions]\n",
    "                    random_state.shuffle(actions)\n",
    "                    action = actions[0]\n",
    "                    print(\"EXPLORING ACTION PERFORMED\", action)\n",
    "                    next_state = action #choose random action\n",
    "\n",
    "                    print(\"next_state1\", next_state)\n",
    "                    state_space(next_state)\n",
    "\n",
    "                else: #Exploit Q table\n",
    "                    print(\"EXPLOITING\")\n",
    "                    if np.sum(q[current_state]) > 0:\n",
    "                        action = np.argmax(q[current_state]) #action is the max of the q-val in the current state\n",
    "                    else:\n",
    "                        # Don't allow invalid moves at the start\n",
    "                        # Just take a random move\n",
    "                        actions = np.array(list(range(n_actions-1)))\n",
    "                        actions = actions[valid_moves == True]\n",
    "                        random_state.shuffle(actions)\n",
    "                        action = actions[0]\n",
    "                        print(\"EXPLOITING ACTION PERFORMED\", action)\n",
    "\n",
    "\n",
    "                    next_state = action\n",
    "                    state_space(next_state)\n",
    "\n",
    "                reward = update_q(current_state, next_state, action)\n",
    "                print(\"reward\", reward)\n",
    "                \n",
    "                # Goal state has reward 100\n",
    "                if reward > 1:\n",
    "                    goal = True\n",
    "                current_state = next_state\n",
    "\n",
    "    #     if (current_state == 1):\n",
    "    #         print(\"STATE 2\")\n",
    "\n",
    "    #         while not goal: \n",
    "    #             # epsilon greedy\n",
    "    #             valid_moves = r[current_state] >= 0\n",
    "    #             print(\"current_state\", current_state)\n",
    "    #             print(\"r[current_state]\", r[current_state])\n",
    "\n",
    "    #             if rm.uniform(0, 1) < epsilon: #Explore\n",
    "    #                 print(\"EXPLORING\")\n",
    "    #                 actions = np.array(list(range(n_actions)))\n",
    "    #                 actions = actions[valid_moves == True]\n",
    "    #                 print(\"CHECKING VALID ACTIONS\", actions)\n",
    "    #                 if type(actions) is int:\n",
    "    #                     actions = [actions]\n",
    "    #                 random_state.shuffle(actions)\n",
    "    #                 action = actions[0]\n",
    "    #                 print(\"FIRST ACTION PERFORMED\", action)\n",
    "    #                 next_state = action #choose random action\n",
    "\n",
    "    #                 print(\"next_state1\", next_state)\n",
    "\n",
    "    #             else: #Exploit Q table\n",
    "    #                 print(\"EXPLOITING\")\n",
    "    #                 if np.sum(q[current_state]) > 0:\n",
    "    #                     action = np.argmax(q[current_state]) #action is the max of the q-val in the current state\n",
    "    #                 else:\n",
    "    #                     # Don't allow invalid moves at the start\n",
    "    #                     # Just take a random move\n",
    "    #                     actions = np.array(list(range(n_actions)))\n",
    "    #                     actions = actions[valid_moves == True]\n",
    "    #                     random_state.shuffle(actions)\n",
    "    #                     action = actions[0]\n",
    "\n",
    "\n",
    "    #                 next_state = action\n",
    "\n",
    "    #             reward = update_q(current_state, next_state, action)\n",
    "    #             print(\"reward\", reward)\n",
    "    #             # Goal state has reward 100\n",
    "    #             if reward > 1:\n",
    "    #                 goal = True\n",
    "    #             current_state = next_state\n",
    "\n",
    "    print(q)\n",
    "    show_traverse()\n",
    "    #show_q()\n",
    "    \n",
    "mdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploiting\n",
      "np.sum(q[current_state]) 1.0\n",
      "np.argmax(Q[state]) 0\n",
      "Invalid Action in State 1!\n",
      "GAME OVER\n",
      "StatesxActions Grid:  [[0, 0], [0, 0], [0, 0]]\n",
      "StatesxActions Reward:  [[0, 0], [0, 0], [0, 0]]\n",
      "Q Matrix [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize params\n",
    "gamma = 0.8 # Discount factor \n",
    "alpha = 0.9 # Learning rate \n",
    "\n",
    "n_episodes = 100\n",
    "n_states = 2\n",
    "n_actions = 3\n",
    "epsilon = 0.2\n",
    "\n",
    "state_grid = [[0 for i in range(n_states)] for i in range(n_actions)] #2States x 3 Actions\n",
    "reward_grid = [[0 for i in range(n_states)] for i in range(n_actions)] #2States x 3 Actions\n",
    "\n",
    "\n",
    "Q = np.matrix(np.zeros([n_states,n_actions]))\n",
    "\n",
    "def mdp_game(s):\n",
    "       \n",
    "    if( s == 1 ):\n",
    "        \n",
    "        if rm.uniform(0, 1) < epsilon:\n",
    "            \"\"\"\n",
    "            Explore:\n",
    "            \"\"\"\n",
    "            #Automate the choice for action\n",
    "            print(\"Exploring\")\n",
    "            list1=['a', 'b', 'c']\n",
    "            b=rm.randint(0,2) #.choice\n",
    "            action = list1[b]\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Exploit: select the action with max value\n",
    "            \"\"\"\n",
    "            print(\"Exploiting\")\n",
    "            \n",
    "            if np.sum(q[current_state]) > 0:\n",
    "                print(\"np.sum(q[current_state])\", np.sum(q[current_state]))\n",
    "                action = np.argmax(Q[s])\n",
    "                print(\"np.argmax(Q[state])\", np.argmax(Q[s]))\n",
    "            else:\n",
    "                # Don't allow invalid moves take a random move\n",
    "                list1=['a', 'b', 'c']\n",
    "                b=rm.randint(0,2)\n",
    "                action = list1[b]\n",
    "\n",
    "        if( action == ('A') or action == ('a')):\n",
    "            state_grid[0][0] += 1\n",
    "\n",
    "            if( Lev >= 75):\n",
    "                reward_grid[0][0] += 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[0][0] -= .25\n",
    "                prize.append(r)\n",
    "\n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            state_grid[0][1] += 1\n",
    "\n",
    "            if( Lev >= 75):\n",
    "                reward_grid[0][1] -= 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward_grid[0][1] += .25\n",
    "                prize.append(r)\n",
    "\n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            state_grid[0][2] += 1\n",
    "            mdp_game()\n",
    "\n",
    "            if( Lev >= 75):\n",
    "                reward_grid[0][2] += .25\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward_grid[0][2] -= .25\n",
    "                prize.append(r)\n",
    "        else:\n",
    "            print(\"Invalid Action in State 1!\")\n",
    "\n",
    "    elif( s == 2 ):\n",
    "        if rm.uniform(0, 1) < epsilon:\n",
    "            \"\"\"\n",
    "            Explore:\n",
    "            \"\"\"\n",
    "            #Automate the choice for action\n",
    "            list1=['a', 'b', 'c']\n",
    "            b=rm.randint(0,2)\n",
    "            action = list1[b]\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Exploit: select the action with max value\n",
    "            \"\"\"\n",
    "            if np.sum(q[current_state]) > 0:\n",
    "                action = np.argmax(Q[s])\n",
    "            else:\n",
    "                # Don't allow invalid moves take a random move\n",
    "                list1=['a', 'b', 'c']\n",
    "                b=rm.randint(0,2)\n",
    "                action = list1[b]\n",
    "\n",
    "\n",
    "        if( action == ('A') or action == ('a')):\n",
    "            state_grid[1][0] += 1\n",
    "\n",
    "            if( Lev >= 75):\n",
    "                reward_grid[1][0] += 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward_grid[1][0] -= .25\n",
    "                prize.append(r)\n",
    "\n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            state_grid[1][1] += 1\n",
    "\n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                reward_grid[1][1] -= 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward_grid[1][1] += .25\n",
    "                prize.append(r)\n",
    "\n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            state_grid[1][2] += 1\n",
    "\n",
    "            if( Lev >= 75):\n",
    "                reward_grid[1][2] += .25\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward_grid[1][2] -= .25\n",
    "                prize.append(r)\n",
    "\n",
    "            mdp_game()\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid Action in State 2!\")\n",
    "    else:\n",
    "        print(\"Invalid State!\") \n",
    "       \n",
    "    \n",
    "while True:\n",
    "\n",
    "    mdp_game(1)\n",
    "   # restart = input('do you want to restart Y/N?')\n",
    "\n",
    "    #Automate Restart of game\n",
    "    list1=['N', 'Y']\n",
    "    b=rm.randint(0,1)\n",
    "    restart = list1[b]\n",
    "\n",
    "    if restart == 'N' or restart == 'n':\n",
    "        print(\"GAME OVER\")\n",
    "        break\n",
    "    elif restart == 'Y' or restart == 'y':\n",
    "        print(\"NEW GAME\")\n",
    "        #mdp_game()\n",
    "\n",
    "print(\"StatesxActions Grid: \", state_grid)\n",
    "print(\"StatesxActions Reward: \", reward_grid)   \n",
    "print(\"Q Matrix\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = 2\n",
    "cols = 3\n",
    "grid = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "\n",
    "\n",
    "\n",
    "# Initialize params\n",
    "gamma = 0.8 # Discount factor \n",
    "alpha = 0.9 # Learning rate \n",
    "\n",
    "n_episodes = 100\n",
    "n_states = 2\n",
    "n_actions = 3\n",
    "epsilon = 0.2\n",
    "\n",
    "Q = np.matrix(np.zeros([n_states,n_actions]))\n",
    "#Keeping track of how reward is calculated\n",
    "Lev = Levenshtein\n",
    "\n",
    "#For the reward, I have chosen a gradually increasing reward as the goal is approached with obstacles based on the Lev score\n",
    "reward = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "# rewards = [[1, -1, .33], \n",
    "#            [1, -1, .33]]\n",
    "\n",
    "print(\"Decision Modeling MDP Game\")\n",
    "print(\"===========================================\")\n",
    "print(\"Follow the prompted Instructions\")\n",
    "    \n",
    "def mdp_game(s):\n",
    "    r = 0\n",
    "#    Q[state, action] = old_value + alpha * (reward + gamma * max(Q[next_state, :]) - old_value)\n",
    "\n",
    "    \n",
    "#----------------------Q-learning Process----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "    #Automate the choice for state\n",
    "\n",
    "    state = rm.randint(0,1) #before for loop\n",
    "\n",
    "    if( s == 1 ):\n",
    "        print(\"You have selected Education\")\n",
    "\n",
    "        print(\"\\nWhich action would you like to take?\")\n",
    "        print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "        #action = input(\"Which action would you like to accomplish to the candidate's application? \\n A) Accept\\n B) Decline\\n C) Review\\n Enter A or B or C:\")\n",
    "\n",
    "        if rm.uniform(0, 1) < epsilon:\n",
    "            \"\"\"\n",
    "            Explore:\n",
    "            \"\"\"\n",
    "            #Automate the choice for action\n",
    "            print(\"Exploring\")\n",
    "            list1=['a', 'b', 'c']\n",
    "            b=rm.randint(0,2) #.choice\n",
    "            action = list1[b]\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Exploit: select the action with max value\n",
    "            \"\"\"\n",
    "            print(\"Exploiting\")\n",
    "            if np.sum(q[state]) > 0:\n",
    "                print(\"np.sum(q[current_state])\", np.sum(q[current_state]))\n",
    "                action = np.argmax(Q[state])\n",
    "                print(\"np.argmax(Q[state])\", np.argmax(Q[state]))\n",
    "            else:\n",
    "                # Don't allow invalid moves take a random move\n",
    "                list1=['a', 'b', 'c']\n",
    "                b=rm.randint(0,2)\n",
    "                action = list1[b]\n",
    "\n",
    "\n",
    "\n",
    "        if( action == ('A') or action == ('a')):\n",
    "            print(\"You have chosen to accept the candidate's application\")\n",
    "            print(\"Accept\")\n",
    "\n",
    "            grid[0][0] += 1\n",
    "\n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                reward[0][0] += 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[0][0] -= .25\n",
    "                prize.append(r)\n",
    "\n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            print( \"You have chosen to decline the candidate's application\" )\n",
    "            print(\"Decline\")\n",
    "\n",
    "            grid[0][1] += 1\n",
    "\n",
    "            #Keeping track of reward\n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                reward[0][1] -= 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[0][1] += .25\n",
    "                prize.append(r)\n",
    "\n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "            print(\"Continue\")\n",
    "            grid[0][2] += 1\n",
    "            mdp_game()\n",
    "\n",
    "            #Keeping track of reward\n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                reward[0][2] += .25\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[0][2] -= .25\n",
    "                prize.append(r)\n",
    "        else:\n",
    "            print(\"Invalid Action in State 1!\")\n",
    "            \n",
    "\n",
    "\n",
    "    elif( s == 2 ):\n",
    "        print( \"You have selected Skills\" )\n",
    "\n",
    "        print(\"\\nWhich action would you like to take?\")\n",
    "        print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "      #  action = input(\"Which action would you like to accomplish to the candidate's application? \\n A) Accept\\n B) Decline\\n C) Review\\n Enter A or B or C:\")\n",
    "\n",
    "        if rm.uniform(0, 1) < epsilon:\n",
    "            \"\"\"\n",
    "            Explore:\n",
    "            \"\"\"\n",
    "            #Automate the choice for action\n",
    "            list1=['a', 'b', 'c']\n",
    "            b=rm.randint(0,2)\n",
    "            action = list1[b]\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Exploit: select the action with max value\n",
    "            \"\"\"\n",
    "            if np.sum(q[current_state]) > 0:\n",
    "                action = np.argmax(Q[state])\n",
    "            else:\n",
    "                # Don't allow invalid moves take a random move\n",
    "                list1=['a', 'b', 'c']\n",
    "                b=rm.randint(0,2)\n",
    "                action = list1[b]\n",
    "\n",
    "\n",
    "        if( action == ('A') or action == ('a')):\n",
    "            print(\"You have chosen to accept the candidate's application\")\n",
    "            print(\"Accept\")\n",
    "\n",
    "            grid[1][0] += 1\n",
    "\n",
    "            if( Lev >= 75):\n",
    "                reward[1][0] += 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[1][0] -= .25\n",
    "                prize.append(r)\n",
    "\n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            print( \"You have chosen to decline the candidate's application\" )\n",
    "            print(\"Decline\")\n",
    "\n",
    "            grid[1][1] += 1\n",
    "\n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                reward[1][1] -= 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[1][1] += .25\n",
    "                prize.append(r)\n",
    "\n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "            print(\"Continue\")\n",
    "            grid[1][2] += 1\n",
    "\n",
    "            if( Lev >= 75):\n",
    "                reward[1][2] += .25\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[1][2] -= .25\n",
    "                prize.append(r)\n",
    "\n",
    "            mdp_game()\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid Action in State 2!\")\n",
    "    else:\n",
    "        print(\"Invalid State!\")   \n",
    "\n",
    "if i % 100 == 0:\n",
    "    print(f\"Episode: {i}\")\n",
    "\n",
    "#     print(\"Reward: \", reward)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    mdp_game(1)\n",
    "   # restart = input('do you want to restart Y/N?')\n",
    "\n",
    "    #Automate Restart of game\n",
    "    list1=['N', 'Y']\n",
    "    b=rm.randint(0,1)\n",
    "    restart = list1[b]\n",
    "\n",
    "    if restart == 'N' or restart == 'n':\n",
    "        print(\"GAME OVER\")\n",
    "        break\n",
    "    elif restart == 'Y' or restart == 'y':\n",
    "        print(\"NEW GAME\")\n",
    "        #mdp_game()\n",
    "\n",
    "\n",
    "def mdp_experiment():\n",
    "    for i in range(1000):\n",
    "        mdp_game(choice())\n",
    "print(\"StatesxActions Grid: \", grid)\n",
    "print(\"StatesxActions Reward: \", reward)\n",
    "#print(\"reward:\", sum(reward))\n",
    "\n",
    "\n",
    "old_value = Q[1, 1]\n",
    "#next_state = \n",
    "        \n",
    "new_q = old_value + alpha * (reward + gamma * max(Q[next_state, :]) - old_value)\n",
    "\n",
    "# Q[state, action] = new_q\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Version of Working Q Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "rows = 2\n",
    "cols = 3\n",
    "grid = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "\n",
    "\n",
    "\n",
    "#Keeping track of how reward is calculated\n",
    "Lev = Levenshtein\n",
    "\n",
    "#For the reward, I have chosen a gradually increasing reward as the goal is approached with obstacles based on the Lev score\n",
    "reward_grid = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "reward = []\n",
    "prize = []\n",
    "\n",
    "print(\"Decision Modeling MDP Choice Game\")\n",
    "print(\"===========================================\")\n",
    "print(\"Follow the prompted Instructions\")\n",
    "    \n",
    "def mdp_game():\n",
    "    r = 0\n",
    "    \n",
    "    #Automate the choice for state\n",
    "    state = rm.randint(1,2)\n",
    "    if( state == 1 ):\n",
    "        print(\"You have selected Education\")\n",
    "        \n",
    "        print(\"\\nWhich action would you like to take?\")\n",
    "        print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "        #action = input(\"Which action would you like to accomplish to the candidate's application? \\n A) Accept\\n B) Decline\\n C) Review\\n Enter A or B or C:\")\n",
    "        \n",
    "        #Automate the choice for action\n",
    "        list1=['a', 'b', 'c']\n",
    "        b=rm.randint(0,2)\n",
    "        action = list1[b]\n",
    "        \n",
    "        if( action == ('A') or action == ('a')):\n",
    "            print(\"You have chosen to accept the candidate's application\")\n",
    "            print(\"Accept\")\n",
    "            \n",
    "            grid[0][0] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                r += 1\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r -= .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            reward_grid[0][0] += r\n",
    "                \n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            print( \"You have chosen to decline the candidate's application\" )\n",
    "            print(\"Decline\")\n",
    "           \n",
    "            grid[0][1] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                r -= 1\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r += .25\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            reward_grid[0][1] += r\n",
    "                \n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "            print(\"Continue\")\n",
    "            grid[0][2] += 1\n",
    "            mdp_game()\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 50):\n",
    "                r += .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r -= .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            reward_grid[0][2] += r \n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid choice!\")\n",
    "\n",
    "        \n",
    "    elif( state == 2 ):\n",
    "        print( \"You have selected Skills\" )\n",
    "        \n",
    "        print(\"\\nWhich action would you like to take?\")\n",
    "        print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "        #Automate the choice for action\n",
    "        list1=['a', 'b', 'c']\n",
    "        b=rm.randint(0,2)\n",
    "        action = list1[b]\n",
    "        \n",
    "        if( action == ('A') or action == ('a')):\n",
    "            print(\"You have chosen to accept the candidate's application\")\n",
    "            print(\"Accept\")\n",
    "            \n",
    "            grid[1][0] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                r += 1\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r -= .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "                \n",
    "            reward_grid[1][0] += r \n",
    "                \n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            print( \"You have chosen to decline the candidate's application\" )\n",
    "            print(\"Decline\")\n",
    "            \n",
    "            grid[1][1] += 1\n",
    "            \n",
    "            if( Lev >= 75):\n",
    "                r -= 1\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r += .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            \n",
    "            reward_grid[1][1] += r \n",
    "                \n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "            print(\"Continue\")\n",
    "            grid[1][2] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 50):\n",
    "                r += .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r -= .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "\n",
    "            reward_grid[1][2] += r\n",
    "                \n",
    "            mdp_game()\n",
    "                \n",
    "        else:\n",
    "            print(\"Invalid choice!\")\n",
    "    else:\n",
    "        print(\"Invalid choice!\")   \n",
    "        \n",
    "\n",
    "while True:\n",
    "    \n",
    "        mdp_game()\n",
    "        \n",
    "        #Automate Restart of game\n",
    "        list1=['N', 'Y']\n",
    "        b=rm.randint(0,1)\n",
    "        restart = list1[b]\n",
    "    \n",
    "        if restart == 'N' or restart == 'n':\n",
    "            print(\"GAME OVER\")\n",
    "            break\n",
    "        elif restart == 'Y' or restart == 'y':\n",
    "            print(\"NEW GAME\")\n",
    "            #mdp_game()\n",
    "            \n",
    "print(\"StatesxActions Grid: \", grid)\n",
    "print(\"StatesxActions Reward: \", reward)\n",
    "# print(\"reward:\", sum(reward))\n",
    "\n",
    "\n",
    "GAMMA = 0.9 \n",
    "alpha = 1.\n",
    "\n",
    "Q = np.zeros(( 5 , 3 )) #initialize Q\n",
    "\n",
    "Re = np.array(reward_grid)\n",
    "\n",
    "def getMaxQ(state): \n",
    "    return max(Q[state,: ])\n",
    "\n",
    "def QLearning(state): \n",
    "    curAction = None \n",
    "    for action in range(2):\n",
    "        if (Re[state][action] ==- 1 ):\n",
    "            Q[state, action] = 0\n",
    "        else :\n",
    "            curAction = action\n",
    "            #\n",
    "            Q[state, action] = Q[state, action] + alpha * (Re[state][action] + GAMMA * getMaxQ(curAction)-Q[state, action])\n",
    "            \n",
    "\n",
    "count = 0     \n",
    "while count < 1000:\n",
    "    for i in range(2):\n",
    "        QLearning(i)\n",
    "    count += 1 \n",
    "print(Q/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = 2\n",
    "cols = 3\n",
    "grid = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "\n",
    "# Initialize params\n",
    "gamma = 0.8 # Discount factor \n",
    "alpha = 0.9 # Learning rate \n",
    "\n",
    "n_episodes = 100\n",
    "n_states = 2\n",
    "n_actions = 3\n",
    "epsilon = 0.05\n",
    "\n",
    "\n",
    "#Keeping track of how reward is calculated\n",
    "Lev = Levenshtein\n",
    "\n",
    "#For the reward, I have chosen a gradually increasing reward as the goal is approached with obstacles based on the Lev score\n",
    "reward = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "rewards = [[1, -1, .33], \n",
    "           [1, -1, .33]]\n",
    "prize = []\n",
    "\n",
    "print(\"Decision Modeling MDP Choice Game\")\n",
    "print(\"===========================================\")\n",
    "print(\"Follow the prompted Instructions\")\n",
    "    \n",
    "def mdp_game():\n",
    "    r = 0\n",
    "    \n",
    "    pi = []\n",
    "    #state = int( input(\"Please Select which state you would like to examine.\\n Press 1 and hit enter to choose Education.\\n Press 2 and hit enter to choose Skills\") )\n",
    "    \n",
    "    \n",
    "    #Automate the choice for state\n",
    "    state = rm.randint(1,2)\n",
    "    if( state == 1 ):\n",
    "        print(\"You have selected Education\")\n",
    "        \n",
    "        print(\"\\nWhich action would you like to take?\")\n",
    "        print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "        #action = input(\"Which action would you like to accomplish to the candidate's application? \\n A) Accept\\n B) Decline\\n C) Review\\n Enter A or B or C:\")\n",
    "        \n",
    "        #Automate the choice for action\n",
    "        list1=['a', 'b', 'c']\n",
    "        b=rm.randint(0,2)\n",
    "        action = list1[b]\n",
    "        \n",
    "        if( action == ('A') or action == ('a')):\n",
    "            print(\"You have chosen to accept the candidate's application\")\n",
    "            print(\"Accept\")\n",
    "            \n",
    "            grid[0][0] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                reward[0][0] += 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[0][0] -= .25\n",
    "                prize.append(r)\n",
    "                \n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            print( \"You have chosen to decline the candidate's application\" )\n",
    "            print(\"Decline\")\n",
    "           \n",
    "            grid[0][1] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                reward[0][1] -= 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[0][1] += .25\n",
    "                prize.append(r)\n",
    "                \n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "            print(\"Continue\")\n",
    "            grid[0][2] += 1\n",
    "            mdp_game()\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                reward[0][2] += .25\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[0][2] -= .25\n",
    "                prize.append(r)\n",
    "        else:\n",
    "            print(\"Invalid choice!\")\n",
    "\n",
    "        \n",
    "    elif( state == 2 ):\n",
    "        print( \"You have selected Skills\" )\n",
    "        \n",
    "        print(\"\\nWhich action would you like to take?\")\n",
    "        print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "      #  action = input(\"Which action would you like to accomplish to the candidate's application? \\n A) Accept\\n B) Decline\\n C) Review\\n Enter A or B or C:\")\n",
    "\n",
    "        #Automate the choice for action\n",
    "        list1=['a', 'b', 'c']\n",
    "        b=rm.randint(0,2)\n",
    "        action = list1[b]\n",
    "        \n",
    "        if( action == ('A') or action == ('a')):\n",
    "            print(\"You have chosen to accept the candidate's application\")\n",
    "            print(\"Accept\")\n",
    "            \n",
    "            grid[1][0] += 1\n",
    "            \n",
    "            if( Lev >= 75):\n",
    "                reward[1][0] += 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[1][0] -= .25\n",
    "                prize.append(r)\n",
    "                \n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            print( \"You have chosen to decline the candidate's application\" )\n",
    "            print(\"Decline\")\n",
    "            \n",
    "            grid[1][1] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                reward[1][1] -= 1\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[1][1] += .25\n",
    "                prize.append(r)\n",
    "                \n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "            print(\"Continue\")\n",
    "            grid[1][2] += 1\n",
    "            \n",
    "            if( Lev >= 75):\n",
    "                reward[1][2] += .25\n",
    "                prize.append(r)\n",
    "            else:\n",
    "                reward[1][2] -= .25\n",
    "                prize.append(r)\n",
    "                \n",
    "            mdp_game()\n",
    "                \n",
    "        else:\n",
    "            print(\"Invalid choice!\")\n",
    "    else:\n",
    "        print(\"Invalid choice!\")   \n",
    "        \n",
    "#     print(\"Reward: \", reward)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "        mdp_game()\n",
    "       # restart = input('do you want to restart Y/N?')\n",
    "        \n",
    "        #Automate Restart of game\n",
    "        list1=['N', 'Y']\n",
    "        b=rm.randint(0,1)\n",
    "        restart = list1[b]\n",
    "    \n",
    "        if restart == 'N' or restart == 'n':\n",
    "            print(\"GAME OVER\")\n",
    "            break\n",
    "        elif restart == 'Y' or restart == 'y':\n",
    "            print(\"NEW GAME\")\n",
    "            #mdp_game()\n",
    "            \n",
    "print(\"StatesxActions Grid: \", grid)\n",
    "print(\"StatesxActions Reward: \", reward)\n",
    "#print(\"reward:\", sum(reward))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q = np.array(np.zeros(( n_states , n_actions ))) #initialize Q\n",
    "\n",
    "Re = np.array(reward)\n",
    "\n",
    "\n",
    "def QLearning(state): \n",
    "    currAction = None \n",
    "    for action in range(n_actions-1):\n",
    "        if (Re[state][action] ==- 1 ):\n",
    "            Q[state, action] = 0\n",
    "        else :\n",
    "            currAction = action\n",
    "            #\n",
    "            Q[state, action] = Q[state, action] + alpha * (Re[state][action] + gamma * max(Q[curAction,: ]) - Q[state, action])\n",
    "\n",
    "count = 0     \n",
    "while count < 1000:\n",
    "    for i in range(n_states):\n",
    "        QLearning(i)\n",
    "    count += 1 \n",
    "print(\"Q-Matrix\", Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = 2\n",
    "cols = 3\n",
    "grid = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "\n",
    "Q = np.matrix(np.zeros([2,3]))\n",
    "\n",
    "# Initialize params\n",
    "gamma = 0.8 # Discount factor \n",
    "alpha = 0.9 # Learning rate \n",
    "\n",
    "n_episodes = 100\n",
    "n_states = 2\n",
    "n_actions = 3\n",
    "epsilon = 0.2\n",
    "\n",
    "#Keeping track of how reward is calculated\n",
    "Lev = Levenshtein\n",
    "\n",
    "#For the reward, I have chosen a gradually increasing reward as the goal is approached with obstacles based on the Lev score\n",
    "reward = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "# rewards = [[1, -1, .33], \n",
    "#            [1, -1, .33]]\n",
    "prize = []\n",
    "\n",
    "print(\"Decision Modeling MDP Game\")\n",
    "print(\"===========================================\")\n",
    "print(\"Follow the prompted Instructions\")\n",
    "    \n",
    "def mdp_game():\n",
    "    r = 0\n",
    "    \n",
    "    #state = int( input(\"Please Select which state you would like to examine.\\n Press 1 and hit enter to choose Education.\\n Press 2 and hit enter to choose Skills\") )\n",
    "    \n",
    "#----------------------Q-learning Process----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    for i in range(1000):\n",
    "        \n",
    "        #Automate the choice for state\n",
    "\n",
    "        state = rm.randint(0,1) #before for loop\n",
    "    \n",
    "        if( state == 1 ):\n",
    "            print(\"You have selected Education\")\n",
    "\n",
    "            print(\"\\nWhich action would you like to take?\")\n",
    "            print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "            #action = input(\"Which action would you like to accomplish to the candidate's application? \\n A) Accept\\n B) Decline\\n C) Review\\n Enter A or B or C:\")\n",
    "            \n",
    "            if rm.uniform(0, 1) < epsilon:\n",
    "                \"\"\"\n",
    "                Explore:\n",
    "                \"\"\"\n",
    "                #Automate the choice for action\n",
    "                print(\"Exploring\")\n",
    "                list1=['a', 'b', 'c']\n",
    "                b=rm.randint(0,2) #.choice\n",
    "                action = list1[b]\n",
    "            else:\n",
    "                \"\"\"\n",
    "                Exploit: select the action with max value\n",
    "                \"\"\"\n",
    "                print(\"Exploiting\")\n",
    "                if np.sum(q[current_state]) > 0:\n",
    "                    print(\"np.sum(q[current_state])\", np.sum(q[current_state]))\n",
    "                    action = np.argmax(Q[state])\n",
    "                    print(\"np.argmax(Q[state])\", np.argmax(Q[state]))\n",
    "                else:\n",
    "                    # Don't allow invalid moves take a random move\n",
    "                    list1=['a', 'b', 'c']\n",
    "                    b=rm.randint(0,2)\n",
    "                    action = list1[b]\n",
    "                \n",
    "                \n",
    "                \n",
    "            if( action == ('A') or action == ('a')):\n",
    "                print(\"You have chosen to accept the candidate's application\")\n",
    "                print(\"Accept\")\n",
    "\n",
    "                grid[0][0] += 1\n",
    "\n",
    "                #Keeping track of reward\n",
    "                if( Lev >= 75):\n",
    "                    reward[0][0] += 1\n",
    "                    prize.append(r)\n",
    "                else:\n",
    "                    reward[0][0] -= .25\n",
    "                    prize.append(r)\n",
    "\n",
    "            elif( action == (\"B\") or action == (\"b\") ):\n",
    "                print( \"You have chosen to decline the candidate's application\" )\n",
    "                print(\"Decline\")\n",
    "\n",
    "                grid[0][1] += 1\n",
    "\n",
    "                #Keeping track of reward\n",
    "                #Keeping track of reward\n",
    "                if( Lev >= 75):\n",
    "                    reward[0][1] -= 1\n",
    "                    prize.append(r)\n",
    "                else:\n",
    "                    reward[0][1] += .25\n",
    "                    prize.append(r)\n",
    "\n",
    "            elif( action == (\"C\") or action == (\"c\") ):\n",
    "                print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "                print(\"Continue\")\n",
    "                grid[0][2] += 1\n",
    "                mdp_game()\n",
    "\n",
    "                #Keeping track of reward\n",
    "                #Keeping track of reward\n",
    "                if( Lev >= 75):\n",
    "                    reward[0][2] += .25\n",
    "                    prize.append(r)\n",
    "                else:\n",
    "                    reward[0][2] -= .25\n",
    "                    prize.append(r)\n",
    "            else:\n",
    "                print(\"Invalid Action in State 1!\")\n",
    "\n",
    "\n",
    "        elif( state == 2 ):\n",
    "            print( \"You have selected Skills\" )\n",
    "\n",
    "            print(\"\\nWhich action would you like to take?\")\n",
    "            print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "          #  action = input(\"Which action would you like to accomplish to the candidate's application? \\n A) Accept\\n B) Decline\\n C) Review\\n Enter A or B or C:\")\n",
    "\n",
    "            if rm.uniform(0, 1) < epsilon:\n",
    "                \"\"\"\n",
    "                Explore:\n",
    "                \"\"\"\n",
    "                #Automate the choice for action\n",
    "                list1=['a', 'b', 'c']\n",
    "                b=rm.randint(0,2)\n",
    "                action = list1[b]\n",
    "            else:\n",
    "                \"\"\"\n",
    "                Exploit: select the action with max value\n",
    "                \"\"\"\n",
    "                if np.sum(q[current_state]) > 0:\n",
    "                    action = np.argmax(Q[state])\n",
    "                else:\n",
    "                    # Don't allow invalid moves take a random move\n",
    "                    list1=['a', 'b', 'c']\n",
    "                    b=rm.randint(0,2)\n",
    "                    action = list1[b]\n",
    "                \n",
    "                \n",
    "            if( action == ('A') or action == ('a')):\n",
    "                print(\"You have chosen to accept the candidate's application\")\n",
    "                print(\"Accept\")\n",
    "\n",
    "                grid[1][0] += 1\n",
    "\n",
    "                if( Lev >= 75):\n",
    "                    reward[1][0] += 1\n",
    "                    prize.append(r)\n",
    "                else:\n",
    "                    reward[1][0] -= .25\n",
    "                    prize.append(r)\n",
    "\n",
    "            elif( action == (\"B\") or action == (\"b\") ):\n",
    "                print( \"You have chosen to decline the candidate's application\" )\n",
    "                print(\"Decline\")\n",
    "\n",
    "                grid[1][1] += 1\n",
    "\n",
    "                #Keeping track of reward\n",
    "                if( Lev >= 75):\n",
    "                    reward[1][1] -= 1\n",
    "                    prize.append(r)\n",
    "                else:\n",
    "                    reward[1][1] += .25\n",
    "                    prize.append(r)\n",
    "\n",
    "            elif( action == (\"C\") or action == (\"c\") ):\n",
    "                print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "                print(\"Continue\")\n",
    "                grid[1][2] += 1\n",
    "\n",
    "                if( Lev >= 75):\n",
    "                    reward[1][2] += .25\n",
    "                    prize.append(r)\n",
    "                else:\n",
    "                    reward[1][2] -= .25\n",
    "                    prize.append(r)\n",
    "\n",
    "                mdp_game()\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid Action in State 2!\")\n",
    "        else:\n",
    "            print(\"Invalid State!\")   \n",
    "            \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Episode: {i}\")\n",
    "        \n",
    "    #     print(\"Reward: \", reward)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "        mdp_game()\n",
    "       # restart = input('do you want to restart Y/N?')\n",
    "        \n",
    "        #Automate Restart of game\n",
    "        list1=['N', 'Y']\n",
    "        b=rm.randint(0,1)\n",
    "        restart = list1[b]\n",
    "    \n",
    "        if restart == 'N' or restart == 'n':\n",
    "            print(\"GAME OVER\")\n",
    "            break\n",
    "        elif restart == 'Y' or restart == 'y':\n",
    "            print(\"NEW GAME\")\n",
    "            #mdp_game()\n",
    "            \n",
    "print(\"StatesxActions Grid: \", grid)\n",
    "print(\"StatesxActions Reward: \", reward)\n",
    "#print(\"reward:\", sum(reward))\n",
    "\n",
    "\n",
    "old_value = Q[state, action]\n",
    "#next_state = \n",
    "        \n",
    "new_q = old_value + alpha * (reward + gamma * max(Q[next_state, :]) - old_value)\n",
    "\n",
    "Q[state, action] = new_q\n",
    "print(Q[state, action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working Standalone Q Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the param\n",
    "GAMMA = 0.8 \n",
    "Q = np.zeros(( 5 , 3 )) #Q matrix filled with zeros\n",
    "\n",
    "Re = np.array(reward) #reward matrix list -> array\n",
    "\n",
    "\n",
    "#\n",
    "def getMaxQ(state): \n",
    "    return max(Q[state,: ])\n",
    "\n",
    "\n",
    "def QLearning(state): \n",
    "    curAction = None \n",
    "    for action in range(2):\n",
    "        if (Re[state][action] ==- 1 ):\n",
    "            Q[state, action] = 0\n",
    "        else :\n",
    "            curAction = action\n",
    "            Q[state, action] = Re[state][action] + GAMMA * getMaxQ(curAction)\n",
    "\n",
    "count = 0     \n",
    "while count < 1000:\n",
    "    for i in range(2):\n",
    "        QLearning(i)\n",
    "    count += 1 \n",
    "print(Q/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------episode--------- 0\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 2, 4, 0, 3]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "action2 1\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action2 0\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 2\n",
      "next_state1 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "action2 0\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 1\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 0, 1, 4, 2]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "action2 2\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 2\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 4, 3, 2, 1]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 0\n",
      "reward 0.25\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 2\n",
      "next_state1 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 3\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 1, 2, 4, 0]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 4\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 4, 3, 0, 1]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 5\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 0, 2, 1, 4]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 6\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 3, 0, 4, 1]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 7\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 0, 3, 4, 1]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 8\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 4, 1, 2, 0]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 9\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 1, 4, 2, 0]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 10\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 0, 3, 4, 2]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 1\n",
      "reward 0.5\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "action1 2\n",
      "next_state1 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 11\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 1, 4, 3, 0]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 12\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 4, 0, 2, 3]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 13\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 3, 0, 2, 4]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 14\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 4, 1, 3, 0]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 15\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 1, 4, 0, 3]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 16\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 3, 4, 1, 2]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 17\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 0, 4, 1, 2]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 18\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 3, 2, 4, 1]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 19\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 4, 0, 3, 2]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 20\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 1, 4, 0, 3]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 21\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 1, 0, 3, 2]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "action2 2\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 22\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 4, 2, 1, 0]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 23\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 2, 4, 3, 0]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 24\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 1, 4, 3, 0]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 25\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 0, 1, 4, 3]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 26\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 0, 1, 4, 3]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 27\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 1, 0, 3, 2]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 28\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 3, 0, 4, 1]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 29\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 0, 4, 3, 2]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 30\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 0, 2, 3, 1]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 31\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 2, 3, 4, 1]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 32\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 0, 4, 2, 1]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 33\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 4, 2, 3, 1]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 34\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 2, 3, 0, 4]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 35\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 0, 2, 1, 3]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 36\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 2, 4, 0, 3]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 37\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 4, 0, 1, 2]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 38\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 4, 1, 2, 0]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "action1 2\n",
      "next_state1 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 39\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 3, 0, 1, 2]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 0.0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 40\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 1, 3, 0, 2]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 41\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 2, 1, 0, 4]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 42\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 4, 2, 3, 1]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 43\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 2, 3, 1, 0]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 44\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 2, 1, 3, 0]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 45\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 2, 3, 1, 0]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 46\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 0, 2, 3, 4]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 47\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 0, 1, 4, 2]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 48\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 0, 4, 3, 1]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 100.0\n",
      "-----------episode--------- 49\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 4, 0, 2, 1]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 50\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 4, 2, 0, 3]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 51\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 1, 2, 0, 4]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 52\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 3, 0, 1, 4]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 53\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 3, 0, 2, 4]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 54\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 4, 3, 2, 0]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 2\n",
      "next_state1 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 55\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 3, 1, 0, 4]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 56\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 0, 3, 2, 4]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 57\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 1, 3, 4, 0]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 58\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 3, 0, 1, 4]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 59\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 3, 1, 0, 4]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 60\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 1, 0, 2, 4]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 100.0\n",
      "-----------episode--------- 61\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 0, 4, 1, 2]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 62\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 4, 3, 0, 1]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 100.0\n",
      "-----------episode--------- 63\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 3, 1, 2, 4]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 64\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 3, 0, 1, 2]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 65\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 3, 1, 2, 4]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 66\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 0, 2, 3, 4]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 67\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 2, 3, 0, 1]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 68\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 0, 1, 4, 2]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "action1 2\n",
      "next_state1 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 69\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 0, 2, 3, 1]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 70\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 4, 2, 0, 3]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 71\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 1, 2, 0, 3]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 72\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 4, 2, 0, 3]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "action1 2\n",
      "next_state1 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 73\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 0, 3, 2, 4]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 74\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 0, 3, 1, 4]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 75\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 3, 0, 4, 2]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 76\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 4, 1, 0, 2]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 77\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 1, 4, 0, 2]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 78\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 3, 4, 1, 2]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 79\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 4, 3, 0, 2]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 80\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 0, 3, 4, 1]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 81\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 0, 4, 1, 3]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 82\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 3, 2, 4, 1]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 83\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 0, 4, 1, 3]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 84\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 1, 2, 3, 4]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 85\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 4, 2, 1, 0]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 86\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 2, 3, 4, 1]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "action1 0\n",
      "next_state1 0\n",
      "reward 100.0\n",
      "-----------episode--------- 87\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 0, 2, 4, 3]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 88\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 1, 0, 3, 2]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 89\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 1, 2, 4, 3]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 90\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 1, 2, 0, 3]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 91\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 0, 3, 4, 2]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 92\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 1, 0, 3, 4]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 93\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 1, 3, 4, 2]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 94\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 1, 4, 3, 2]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 95\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [0, 4, 1, 3, 2]\n",
      "current_state 0\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 96\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [3, 1, 0, 4, 2]\n",
      "current_state 3\n",
      "r[current_state] [-1. -1.  1.]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 97\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [4, 1, 3, 2, 0]\n",
      "current_state 4\n",
      "r[current_state] [  0.  -1. 100.]\n",
      "next_state2 2\n",
      "reward 100.0\n",
      "-----------episode--------- 98\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [1, 3, 2, 0, 4]\n",
      "current_state 1\n",
      "r[current_state] [0.5 0.5 1. ]\n",
      "next_state2 0\n",
      "reward 0.5\n",
      "r[current_state] [ 0.25 -1.    1.  ]\n",
      "next_state2 2\n",
      "reward 1.0\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "-----------episode--------- 99\n",
      "states [0, 1, 2, 3, 4]\n",
      "states [2, 0, 3, 1, 4]\n",
      "current_state 2\n",
      "r[current_state] [100.  -1.  -1.]\n",
      "next_state2 0\n",
      "reward 100.0\n",
      "[[1.5945131e-10 0.0000000e+00 1.0000000e+00]\n",
      " [9.9868262e-01 2.1802944e-04 1.0992935e-03]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
      " [4.0518692e-23 0.0000000e+00 1.0000000e+00]]\n",
      "Greedy traversal for starting state 0\n",
      "0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0\n",
      "\n",
      "Greedy traversal for starting state 1\n",
      "1 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n",
      "Greedy traversal for starting state 2\n",
      "2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2\n",
      "\n",
      "Greedy traversal for starting state 3\n",
      "3 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0\n",
      "\n",
      "Greedy traversal for starting state 4\n",
      "4 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0 -> 2 -> 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# defines the reward/connection graph\n",
    "r = np.array([[.25, -1,  1],\n",
    "              [ .5, .5, 1],\n",
    "              [ 100, -1,  -1],\n",
    "              [-1,  -1,  1],\n",
    "              [ 0, -1, 100]]).astype(\"float32\")\n",
    "\n",
    "\n",
    "q = np.zeros_like(r)\n",
    "\n",
    "\n",
    "def update_q(state, next_state, action, alpha, gamma):\n",
    "    rsa = r[state, action]\n",
    "    qsa = q[state, action]\n",
    "    new_q = qsa + alpha * (rsa + gamma * max(q[next_state, :]) - qsa)\n",
    "    q[state, action] = new_q\n",
    "    # renormalize row to be between 0 and 1\n",
    "    rn = q[state][q[state] > 0] / np.sum(q[state][q[state] > 0])\n",
    "    q[state][q[state] > 0] = rn\n",
    "    return r[state, action]\n",
    "\n",
    "\n",
    "def show_traverse():\n",
    "    # show all the greedy traversals\n",
    "    for i in range(len(q)):\n",
    "        current_state = i\n",
    "        traverse = \"%i -> \" % current_state\n",
    "        n_steps = 0\n",
    "        while current_state != 5 and n_steps < 20:\n",
    "            next_state = np.argmax(q[current_state])\n",
    "            current_state = next_state\n",
    "            traverse += \"%i -> \" % current_state\n",
    "            n_steps = n_steps + 1\n",
    "        # cut off final arrow\n",
    "        traverse = traverse[:-4]\n",
    "        print(\"Greedy traversal for starting state %i\" % i)\n",
    "        print(traverse)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "gamma = 0.8\n",
    "alpha = 1.\n",
    "n_episodes = 100\n",
    "n_states = 5\n",
    "n_actions = 3\n",
    "epsilon = 0.05\n",
    "random_state = np.random.RandomState(1999)\n",
    "for e in range(int(n_episodes)):\n",
    "    print(\"-----------episode---------\", e)\n",
    "\n",
    "    states = list(range(n_states))\n",
    "    \n",
    "    print(\"states\", states)\n",
    "    \n",
    "    random_state.shuffle(states) #shuffles order of states\n",
    "    current_state = states[0] #assigns current state as a random state\n",
    "    \n",
    "    print(\"states\", states)\n",
    "\n",
    "    \n",
    "    print(\"current_state\", current_state)\n",
    "    \n",
    "    goal = False\n",
    "    if e % int(n_episodes / 10.) == 0 and e > 0:\n",
    "        pass\n",
    "        # uncomment this to see plots each monitoring\n",
    "        #show_traverse()\n",
    "        #show_q()\n",
    "    while not goal: \n",
    "        # epsilon greedy\n",
    "        valid_moves = r[current_state] >= 0\n",
    "        \n",
    "        print(\"r[current_state]\", r[current_state])\n",
    "\n",
    "        if random_state.rand() < epsilon: #Explore\n",
    "            actions = np.array(list(range(n_actions)))\n",
    "            actions = actions[valid_moves == True]\n",
    "            if type(actions) is int:\n",
    "                actions = [actions]\n",
    "            random_state.shuffle(actions)\n",
    "            action = actions[0]\n",
    "            print(\"action1\", action)\n",
    "            next_state = action #choose random action\n",
    "            print(\"next_state1\", next_state)\n",
    "\n",
    "        else: #Exploit Q table\n",
    "            if np.sum(q[current_state]) > 0:\n",
    "                action = np.argmax(q[current_state]) #action is the max of the q-val in the current state\n",
    "            else:\n",
    "                # Don't allow invalid moves at the start\n",
    "                # Just take a random move\n",
    "                actions = np.array(list(range(n_actions)))\n",
    "                actions = actions[valid_moves == True]\n",
    "                random_state.shuffle(actions)\n",
    "                action = actions[0]\n",
    "                print(\"action2\", action)\n",
    "\n",
    "            next_state = action\n",
    "            print(\"next_state2\", next_state)\n",
    "            \n",
    "        reward = update_q(current_state, next_state, action,\n",
    "                          alpha=alpha, gamma=gamma)\n",
    "        print(\"reward\", reward)\n",
    "        # Goal state has reward 100\n",
    "        if reward > 1:\n",
    "            goal = True\n",
    "        current_state = next_state\n",
    "\n",
    "print(q)\n",
    "show_traverse()\n",
    "#show_q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q matrix\n",
    "Q = np.matrix(np.zeros([2,3]))\n",
    "\n",
    "# Gamma \n",
    "gamma = 0.8\n",
    "\n",
    "# Initial state. (Usually to be chosen at random)\n",
    "initial_state = \n",
    "\n",
    "# This function returns all available actions in the state given as an argument\n",
    "def available_actions(state):\n",
    "    current_state_row = Re[state,:]\n",
    "    av_act = np.where(current_state_row >= 0)[]\n",
    "    return av_act\n",
    "\n",
    "# Get available actions in the current state\n",
    "available_act = available_actions(initial_state) \n",
    "\n",
    "# This function chooses at random which action to be performed within the range \n",
    "# of all the available actions.\n",
    "def sample_next_action(available_actions_range):\n",
    "    next_action = int(np.random.choice(available_act,1))\n",
    "    return next_action\n",
    "\n",
    "# Sample next action to be performed\n",
    "action = sample_next_action(available_act)\n",
    "\n",
    "# This function updates the Q matrix according to the path selected and the Q \n",
    "# learning algorithm\n",
    "def update(current_state, action, gamma):\n",
    "    \n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
    "\n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index, size = 1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    max_value = Q[action, max_index]\n",
    "    \n",
    "    # Q learning formula\n",
    "    Q[current_state, action] = Re[current_state, action] + gamma * max_value\n",
    "\n",
    "# Update Q matrix\n",
    "update(initial_state,action,gamma)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Training\n",
    "\n",
    "# Train over 10 000 iterations. (Re-iterate the process above).\n",
    "for i in range(10000):\n",
    "    current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "    available_act = available_actions(current_state)\n",
    "    action = sample_next_action(available_act)\n",
    "    update(current_state,action,gamma)\n",
    "    \n",
    "# Normalize the \"trained\" Q matrix\n",
    "print(\"Trained Q matrix:\")\n",
    "print(Q/np.max(Q)*100)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Testing\n",
    "\n",
    "# Goal state = 5\n",
    "# Best sequence path starting from 2 -> 2, 3, 1, 5\n",
    "\n",
    "current_state = 0\n",
    "steps = [current_state]\n",
    "\n",
    "while current_state != 2:\n",
    "\n",
    "    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]\n",
    "    \n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size = 1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "    \n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index\n",
    "\n",
    "# Print selected sequence of steps\n",
    "print(\"Selected path:\")\n",
    "print(steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://amunategui.github.io/reinforcement-learning/index.html\n",
    "\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "# map cell to cell, add circular cell to goal point\n",
    "points_list = [(0,1), (1,5), (5,6), (5,4), (1,2), (2,3), (2,7)]\n",
    "\n",
    "goal = 1\n",
    "\n",
    "# import networkx as nx\n",
    "# G=nx.Graph()\n",
    "# G.add_edges_from(points_list)\n",
    "# pos = nx.spring_layout(G)\n",
    "# nx.draw_networkx_nodes(G,pos)\n",
    "# nx.draw_networkx_edges(G,pos)\n",
    "# nx.draw_netwo\n",
    "\n",
    "# rkx_labels(G,pos)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# how many points in graph? x points\n",
    "MATRIX_SIZE = 8\n",
    "\n",
    "# create matrix x*y\n",
    "R = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))\n",
    "\n",
    "# state_size = 5\n",
    "# action_size = 3\n",
    "# grid = [[0 for i in range(action_size)] for i in range(state_size)] #2States x 3 Actions\n",
    "R *= -1\n",
    "\n",
    "\n",
    "# assign zeros to paths and 100 to goal-reaching point\n",
    "for point in points_list:\n",
    "    print(point)\n",
    "    if point[1] == goal:\n",
    "        R[point] = 500\n",
    "    else:\n",
    "        R[point] = 0\n",
    "\n",
    "    if point[0] == goal:\n",
    "        R[point[::-1]] = 500\n",
    "    else:\n",
    "        # reverse of point\n",
    "        R[point[::-1]]= 0\n",
    "\n",
    "# add goal point round trip\n",
    "R[goal,goal]= 500\n",
    "\n",
    "print(R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q = np.matrix(np.zeros([5,3]))\n",
    "\n",
    "# learning parameter\n",
    "gamma = 0.8\n",
    "\n",
    "initial_state = 1\n",
    "\n",
    "def available_actions(state):\n",
    "    current_state_row = R[state,]\n",
    "    av_act = np.where(current_state_row >= 0)[1]\n",
    "    return av_act\n",
    "\n",
    "available_act = available_actions(initial_state) \n",
    "\n",
    "def sample_next_action(available_actions_range):\n",
    "    next_action = int(np.random.choice(available_act,1))\n",
    "    return next_action\n",
    "\n",
    "action = sample_next_action(available_act)\n",
    "\n",
    "def update(current_state, action, gamma):\n",
    "    \n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
    "  \n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index, size = 1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    max_value = Q[action, max_index]\n",
    "  \n",
    "    Q[current_state, action] = R[current_state, action] + gamma * max_value\n",
    "    print('max_value', R[current_state, action] + gamma * max_value)\n",
    "  \n",
    "    if (np.max(Q) > 0):\n",
    "        return(np.sum(Q/np.max(Q)*100))\n",
    "    else:\n",
    "        return (0)\n",
    "    \n",
    "update(initial_state, action, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "scores = []\n",
    "for i in range(700):\n",
    "    current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "    available_act = available_actions(current_state)\n",
    "    action = sample_next_action(available_act)\n",
    "    score = update(current_state,action,gamma)\n",
    "    scores.append(score)\n",
    "    print ('Score:', str(score))\n",
    "    \n",
    "print(\"Trained Q matrix:\")\n",
    "print(Q/np.max(Q)*100)\n",
    "\n",
    "# Testing\n",
    "current_state = 0\n",
    "steps = [current_state]\n",
    "\n",
    "while current_state != 7:\n",
    "\n",
    "    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]\n",
    "    \n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size = 1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "    \n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index\n",
    "\n",
    "print(\"Most efficient path:\")\n",
    "print(steps)\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R matrix\n",
    "R = np.matrix([[.25 , -1 ,1 ],\n",
    "        [.5 , .5 , 1 ],\n",
    "       [.5 , .5 , 1 ],\n",
    "       [-1 , -1 ,.5 ],\n",
    "       [ 0 , 0 , .5 ]])\n",
    "\n",
    "# Q matrix\n",
    "Q = np.matrix(np.zeros([2,3]))\n",
    "\n",
    "# Gamma \n",
    "gamma = 0.8\n",
    "\n",
    "# Initial state. (Usually to be chosen at random)\n",
    "initial_state = 1\n",
    "\n",
    "# This function returns all available actions in the state given as an argument\n",
    "def available_actions(state):\n",
    "    current_state_row = Re[state,]\n",
    "    av_act = np.where(current_state_row >= 0)[1]\n",
    "    return av_act\n",
    "\n",
    "# Get available actions in the current state\n",
    "available_act = available_actions(initial_state) \n",
    "\n",
    "# This function chooses at random which action to be performed within the range \n",
    "# of all the available actions.\n",
    "def sample_next_action(available_actions_range):\n",
    "    next_action = int(np.random.choice(available_act,1))\n",
    "    return next_action\n",
    "\n",
    "# Sample next action to be performed\n",
    "action = sample_next_action(available_act)\n",
    "\n",
    "# This function updates the Q matrix according to the path selected and the Q \n",
    "# learning algorithm\n",
    "def update(current_state, action, gamma):\n",
    "    \n",
    "    max_index = np.where(Q[action,] == np.max(Q[action,]))[1]\n",
    "\n",
    "    if max_index.shape[0] > 1:\n",
    "        max_index = int(np.random.choice(max_index, size = 1))\n",
    "    else:\n",
    "        max_index = int(max_index)\n",
    "    max_value = Q[action, max_index]\n",
    "    \n",
    "    # Q learning formula\n",
    "    Q[current_state, action] = Re[current_state, action] + gamma * max_value\n",
    "\n",
    "# Update Q matrix\n",
    "update(initial_state,action,gamma)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Training\n",
    "\n",
    "# Train over 10 000 iterations. (Re-iterate the process above).\n",
    "for i in range(10000):\n",
    "    current_state = np.random.randint(0, int(Q.shape[0]))\n",
    "    available_act = available_actions(current_state)\n",
    "    action = sample_next_action(available_act)\n",
    "    update(current_state,action,gamma)\n",
    "    \n",
    "# Normalize the \"trained\" Q matrix\n",
    "print(\"Trained Q matrix:\")\n",
    "print(Q/np.max(Q)*100)\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Testing\n",
    "\n",
    "# Goal state = 5\n",
    "# Best sequence path starting from 2 -> 2, 3, 1, 5\n",
    "\n",
    "current_state = 2\n",
    "steps = [current_state]\n",
    "\n",
    "while current_state != 5:\n",
    "\n",
    "    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]\n",
    "    \n",
    "    if next_step_index.shape[0] > 1:\n",
    "        next_step_index = int(np.random.choice(next_step_index, size = 1))\n",
    "    else:\n",
    "        next_step_index = int(next_step_index)\n",
    "    \n",
    "    steps.append(next_step_index)\n",
    "    current_state = next_step_index\n",
    "\n",
    "# Print selected sequence of steps\n",
    "print(\"Selected path:\")\n",
    "print(steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "GAMMA = 0.8 \n",
    "Q = np.zeros(( 5 , 3 ))\n",
    "\n",
    "#States: Education, Skills, Experience, Referrals, Certification\n",
    "#Actions: Accept, Decline, Continue\n",
    "R = np.asarray([[.25 , -1 ,1 ],\n",
    "   [.5 , .5 , 1 ],\n",
    "   [.5 , .5 , 1 ],\n",
    "   [-1 , -1 ,.5 ],\n",
    "   [ 0 , 0 , .5 ]])\n",
    "\n",
    "def getMaxQ(state): \n",
    "    return max(Q[state,: ])\n",
    "\n",
    "def QLearning(state): \n",
    "    curAction = None \n",
    "    for action in range(3):\n",
    "        if (R[state][action] ==- 1 ):\n",
    "            Q[state, action] = 0\n",
    "        else :\n",
    "            curAction = action\n",
    "            Q[state, action] = R[state][action] + GAMMA * getMaxQ(curAction)\n",
    "\n",
    "count = 0     \n",
    "while count < 1000:\n",
    "    for i in range(5):\n",
    "        QLearning(i)\n",
    "    count += 1 \n",
    "print(Q/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-774523004187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m                           alpha=alpha, gamma=gamma)\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Goal state has reward 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mgoal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "\n",
    "# defines the reward/connection graph\n",
    "r = np.array([ [.25 , -1 ,1 ],\n",
    "               [.5 , .5 , 1 ],\n",
    "               [.5 , .5 , 1 ],\n",
    "               [-1 , -1 ,.5 ],\n",
    "               [ 0 , 0 , .5 ]]).astype(\"float32\")\n",
    "\n",
    "q = np.zeros_like(r)\n",
    "\n",
    "\n",
    "def update_q(state, next_state, action, alpha, gamma):\n",
    "    rsa = r[state, action]\n",
    "    qsa = q[state, action]\n",
    "    new_q = qsa + alpha * (rsa + gamma * max(q[next_state, :]) - qsa)\n",
    "    q[state, action] = new_q\n",
    "    # renormalize row to be between 0 and 1\n",
    "    rn = q[state][q[state] > 0] / np.sum(q[state][q[state] > 0])\n",
    "    q[state][q[state] > 0] = rn\n",
    "    return r[state, action]\n",
    "\n",
    "\n",
    "def show_traverse():\n",
    "    # show all the greedy traversals\n",
    "    for i in range(len(q)):\n",
    "        current_state = i\n",
    "        traverse = \"%i -> \" % current_state\n",
    "        n_steps = 0\n",
    "        while current_state != 5 and n_steps < 20:\n",
    "            next_state = np.argmax(q[current_state])\n",
    "            current_state = next_state\n",
    "            traverse += \"%i -> \" % current_state\n",
    "            n_steps = n_steps + 1\n",
    "        # cut off final arrow\n",
    "        traverse = traverse[:-4]\n",
    "        print(\"Greedy traversal for starting state %i\" % i)\n",
    "        print(traverse)\n",
    "        print(\"\")\n",
    "        \n",
    "# Core algorithm\n",
    "gamma = 0.8\n",
    "alpha = 1.\n",
    "n_episodes = 100\n",
    "n_states = 4\n",
    "n_actions = 3\n",
    "epsilon = 0.05\n",
    "random_state = np.random.RandomState(1999)\n",
    "for e in range(int(n_episodes)):\n",
    "    states = list(range(n_states))\n",
    "    random_state.shuffle(states)\n",
    "    current_state = states[0]\n",
    "    goal = False\n",
    "    if e % int(n_episodes / 10.) == 0 and e > 0:\n",
    "        pass\n",
    "        # uncomment this to see plots each monitoring\n",
    "        #show_traverse()\n",
    "        #show_q()\n",
    "    while not goal:\n",
    "        # epsilon greedy\n",
    "        valid_moves = r[current_state] >= 0\n",
    "        if random_state.rand() < epsilon:\n",
    "            actions = np.array(list(range(n_actions)))\n",
    "            actions = actions[valid_moves == True]\n",
    "            if type(actions) is int:\n",
    "                actions = [actions]\n",
    "            random_state.shuffle(actions)\n",
    "            action = actions[0]\n",
    "            next_state = action\n",
    "            \n",
    "        else:\n",
    "            if np.sum(q[current_state]) > 0:\n",
    "                action = np.argmax(q[current_state])\n",
    "            else:\n",
    "                # Don't allow invalid moves at the start\n",
    "                # Just take a random move\n",
    "                actions = np.array(list(range(n_actions)))\n",
    "                actions = actions[valid_moves == True]\n",
    "                random_state.shuffle(actions)\n",
    "                action = actions[0]\n",
    "            next_state = action\n",
    "            \n",
    "        reward = update_q(current_state, next_state, action,\n",
    "                          alpha=alpha, gamma=gamma)\n",
    "        # Goal state has reward 100\n",
    "        if reward > 1:\n",
    "            goal = True\n",
    "        current_state = next_state\n",
    "\n",
    "print(q)\n",
    "show_traverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kyle Kastner\n",
    "# License: BSD 3-Clause\n",
    "# Implementing http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    "# Q-learning formula from http://sarvagyavaish.github.io/FlappyBirdRL/\n",
    "# Visualization based on code from Gael Varoquaux gael.varoquaux@normalesup.org\n",
    "# http://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "\n",
    "# defines the reward/connection graph\n",
    "r = np.array([[.25, -1,  1],\n",
    "              [ .5, .5, 1],\n",
    "              [ 100, -1,  -1],\n",
    "              [-1,  -1,  1],\n",
    "              [ 0, -1, 100]]).astype(\"float32\")\n",
    "q = np.zeros_like(r)\n",
    "\n",
    "\n",
    "def update_q(state, next_state, action, alpha, gamma):\n",
    "    rsa = r[state, action]\n",
    "    qsa = q[state, action]\n",
    "    new_q = qsa + alpha * (rsa + gamma * max(q[next_state, :]) - qsa)\n",
    "    q[state, action] = new_q\n",
    "    # renormalize row to be between 0 and 1\n",
    "    rn = q[state][q[state] > 0] / np.sum(q[state][q[state] > 0])\n",
    "    q[state][q[state] > 0] = rn\n",
    "    return r[state, action]\n",
    "\n",
    "\n",
    "def show_traverse():\n",
    "    # show all the greedy traversals\n",
    "    for i in range(len(q)):\n",
    "        current_state = i\n",
    "        traverse = \"%i -> \" % current_state\n",
    "        n_steps = 0\n",
    "        while current_state != 5 and n_steps < 20:\n",
    "            next_state = np.argmax(q[current_state])\n",
    "            current_state = next_state\n",
    "            traverse += \"%i -> \" % current_state\n",
    "            n_steps = n_steps + 1\n",
    "        # cut off final arrow\n",
    "        traverse = traverse[:-4]\n",
    "        print(\"Greedy traversal for starting state %i\" % i)\n",
    "        print(traverse)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "# def show_q():\n",
    "#     # show all the valid/used transitions\n",
    "#     coords = np.array([[2, 2],\n",
    "#                        [4, 2],\n",
    "#                        [5, 3],\n",
    "#                        [4, 4],\n",
    "#                        [2, 4],\n",
    "#                        [5, 2]])\n",
    "#     # invert y axis for display\n",
    "#     coords[:, 1] = max(coords[:, 1]) - coords[:, 1]\n",
    "\n",
    "#     plt.figure(1, facecolor='w', figsize=(10, 8))\n",
    "#     plt.clf()\n",
    "#     ax = plt.axes([0., 0., 1., 1.])\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.scatter(coords[:, 0], coords[:, 1], c='r')\n",
    "\n",
    "#     start_idx, end_idx = np.where(q > 0)\n",
    "#     segments = [[coords[start], coords[stop]]\n",
    "#                 for start, stop in zip(start_idx, end_idx)]\n",
    "#     values = np.array(q[q > 0])\n",
    "#     # bump up values for viz\n",
    "#     values = values\n",
    "#     lc = LineCollection(segments,\n",
    "#                         zorder=0, cmap=plt.cm.hot_r)\n",
    "#     lc.set_array(values)\n",
    "#     ax.add_collection(lc)\n",
    "\n",
    "#     verticalalignment = 'top'\n",
    "#     horizontalalignment = 'left'\n",
    "#     for i in range(len(coords)):\n",
    "#         x = coords[i][0]\n",
    "#         y = coords[i][1]\n",
    "#         name = str(i)\n",
    "#         if i == 1:\n",
    "#             y = y - .05\n",
    "#             x = x + .05\n",
    "#         elif i == 3:\n",
    "#             y = y - .05\n",
    "#             x = x + .05\n",
    "#         elif i == 4:\n",
    "#             y = y - .05\n",
    "#             x = x + .05\n",
    "#         else:\n",
    "#             y = y + .05\n",
    "#             x = x + .05\n",
    "\n",
    "#         plt.text(x, y, name, size=10,\n",
    "#                  horizontalalignment=horizontalalignment,\n",
    "#                  verticalalignment=verticalalignment,\n",
    "#                  bbox=dict(facecolor='w',\n",
    "#                            edgecolor=plt.cm.spectral(float(len(coords))),\n",
    "#                            alpha=.6))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Core algorithm\n",
    "gamma = 0.8\n",
    "alpha = 1.\n",
    "n_episodes = 100\n",
    "n_states = 5\n",
    "n_actions = 3\n",
    "epsilon = 0.05\n",
    "random_state = np.random.RandomState(1999)\n",
    "for e in range(int(n_episodes)):\n",
    "    states = list(range(n_states))\n",
    "    random_state.shuffle(states)\n",
    "    current_state = states[0]\n",
    "    goal = False\n",
    "    if e % int(n_episodes / 10.) == 0 and e > 0:\n",
    "        pass\n",
    "        # uncomment this to see plots each monitoring\n",
    "        #show_traverse()\n",
    "        #show_q()\n",
    "    while not goal:\n",
    "        # epsilon greedy\n",
    "        valid_moves = r[current_state] >= 0\n",
    "        if random_state.rand() < epsilon:\n",
    "            actions = np.array(list(range(n_actions)))\n",
    "            actions = actions[valid_moves == True]\n",
    "            if type(actions) is int:\n",
    "                actions = [actions]\n",
    "            random_state.shuffle(actions)\n",
    "            action = actions[0]\n",
    "            next_state = action\n",
    "        else:\n",
    "            if np.sum(q[current_state]) > 0:\n",
    "                action = np.argmax(q[current_state])\n",
    "            else:\n",
    "                # Don't allow invalid moves at the start\n",
    "                # Just take a random move\n",
    "                actions = np.array(list(range(n_actions)))\n",
    "                actions = actions[valid_moves == True]\n",
    "                random_state.shuffle(actions)\n",
    "                action = actions[0]\n",
    "            next_state = action\n",
    "        reward = update_q(current_state, next_state, action,\n",
    "                          alpha=alpha, gamma=gamma)\n",
    "        # Goal state has reward 100\n",
    "        if reward > 1:\n",
    "            goal = True\n",
    "        current_state = next_state\n",
    "\n",
    "print(q)\n",
    "show_traverse()\n",
    "#show_q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archived Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_size = 5\n",
    "action_size = 3\n",
    "grid = [[0 for i in range(action_size)] for i in range(state_size)] #2States x 3 Actions\n",
    "reward = [[0 for i in range(action_size)] for i in range(state_size)] #2States x 3 Actions\n",
    "\n",
    "itr = [0]\n",
    "#Keeping track of how reward is calculated\n",
    "Lev = Levenshtein\n",
    "\n",
    "#For the reward, I have chosen a gradually increasing reward as the goal is approached with obstacles based on the Lev score\n",
    "reward = []\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)\n",
    "\n",
    "\n",
    "# define hyperparameters ----------\n",
    "total_episodes = 15000        # Total episodes\n",
    "learning_rate = 0.8           # Learning rate\n",
    "max_steps = 99                # Max steps per episode\n",
    "gamma = 0.95                  # Discounting rate\n",
    "\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability\n",
    "decay_rate = 0.005             # Exponential decay rate for exploration prob\n",
    "\n",
    "\n",
    "print(\"Decision Modeling MDP Choice Game\")\n",
    "print(\"===========================================\")\n",
    "print(\"Follow the prompted Instructions\")\n",
    "def mdp_game():\n",
    "    # 2 For life or until learning is stopped\n",
    "    for episode in range(total_episodes):\n",
    "        itr[0] += 1\n",
    "        r = 0\n",
    "        \n",
    "        #state = int( input(\"Please Select which state you would like to examine.\\n Press 1 and hit enter to choose Education.\\n Press 2 and hit enter to choose Skills\\n Press 3 and hit enter to choose Experience\\n Press 4 and hit enter to choose Certifications\\n Press 5 and hit enter to choose Referrals\") )\n",
    "\n",
    "        state = rm.randint(1,2)\n",
    "        step = 0\n",
    "        done = False\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        for step in range(max_steps): \n",
    "\n",
    "            if( state == 1 ):\n",
    "                list1=['a', 'b', 'c']\n",
    "                exp_exp_tradeoff=rm.randint(0,2)\n",
    "                action = list1[exp_exp_tradeoff]\n",
    "\n",
    "                if( action == ('A') or action == ('a')):\n",
    "                    print(\"You have chosen to accept the candidate's application\")\n",
    "                    print(\"Accept\")\n",
    "\n",
    "                    grid[0][0] += 1\n",
    "\n",
    "                    #Keeping track of reward\n",
    "                    if( Lev >= 75):\n",
    "                        r += 1\n",
    "                        reward.append(r)\n",
    "                        reward[0][0] += 1\n",
    "                    else:\n",
    "                        r -= .33\n",
    "                        reward.append(r)\n",
    "                        reward[0][0] += 1\n",
    "\n",
    "                elif( action == (\"B\") or action == (\"b\") ):\n",
    "                    print( \"You have chosen to decline the candidate's application\" )\n",
    "                    print(\"Decline\")\n",
    "\n",
    "                    grid[0][1] += 1\n",
    "\n",
    "                    #Keeping track of reward\n",
    "                    if( Lev >= 75):\n",
    "                        r -= 1\n",
    "                        reward.append(r)\n",
    "                        reward[0][1] += 1\n",
    "                    else:\n",
    "                        r += .33\n",
    "                        reward.append(r)\n",
    "                        reward[0][1] += 1\n",
    "\n",
    "                elif( action == (\"C\") or action == (\"c\") ):\n",
    "                    print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "                    print(\"Continue\")\n",
    "                    grid[0][2] += 1\n",
    "                    mdp_game()\n",
    "\n",
    "                    #Keeping track of reward\n",
    "                    if( Lev >= 50):\n",
    "                        r += .33\n",
    "                        reward.append(r)\n",
    "                        reward[0][2] += 1\n",
    "                    else:\n",
    "                        r -= .33\n",
    "                        reward.append(r)\n",
    "                        reward[0][2] += 1\n",
    "                else:\n",
    "                    print(\"Invalid Action Chosen!\")\n",
    "\n",
    "            elif( state == 2 ):\n",
    "\n",
    "                #i += 1\n",
    "               # print(\"ith\", i)\n",
    "                print( \"You have selected Skills\" )\n",
    "\n",
    "                print(\"\\nWhich action would you like to take?\")\n",
    "                print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "              #  action = input(\"Which action would you like to accomplish to the candidate's application? \\n A) Accept\\n B) Decline\\n C) Review\\n Enter A or B or C:\")\n",
    "\n",
    "                list1=['a', 'b', 'c']\n",
    "                b=rm.randint(0,2)\n",
    "                action = list1[b]\n",
    "\n",
    "                if( action == ('A') or action == ('a')):\n",
    "                    print(\"You have chosen to accept the candidate's application\")\n",
    "                    print(\"Accept\")\n",
    "\n",
    "                    grid[1][0] += 1\n",
    "\n",
    "                    #Keeping track of reward\n",
    "                    if( Lev >= 75):\n",
    "                        r += 1\n",
    "                        reward.append(r)\n",
    "                        reward[0][0] += 1\n",
    "                    else:\n",
    "                        r -= .33\n",
    "                        reward.append(r)\n",
    "                        reward[0][0] += 1\n",
    "\n",
    "                elif( action == (\"B\") or action == (\"b\") ):\n",
    "                    print( \"You have chosen to decline the candidate's application\" )\n",
    "                    print(\"Decline\")\n",
    "\n",
    "                    grid[1][1] += 1\n",
    "\n",
    "                    #Keeping track of reward\n",
    "                    if( Lev >= 75):\n",
    "                        r -= 1\n",
    "                        reward.append(r)\n",
    "                        reward[0][1] += 1\n",
    "                    else:\n",
    "                        r += .33\n",
    "                        reward.append(r)\n",
    "                        reward[0][1] += 1\n",
    "\n",
    "                elif( action == (\"C\") or action == (\"c\") ):\n",
    "                    print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "                    print(\"Continue\")\n",
    "                    grid[1][2] += 1\n",
    "                    mdp_game()\n",
    "\n",
    "                    #Keeping track of reward\n",
    "                    if( Lev >= 50):\n",
    "                        r += .33\n",
    "                        reward.append(r)\n",
    "                        reward[0][2] += 1\n",
    "                    else:\n",
    "                        r -= .33\n",
    "                        reward.append(r)\n",
    "                        reward[0][2] += 1\n",
    "                else:\n",
    "                    print(\"Invalid Action Chosen!\")\n",
    "            else:\n",
    "                print(\"Invalid State!\")\n",
    "\n",
    "            if done == True:\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "#     print(\"Reward: \", reward)\n",
    "# else:\n",
    "#     print(\"Single Game Iterations Completed\")\n",
    "#     return\n",
    "\n",
    "\n",
    "while True:\n",
    "    #restart = input('do you want to restart Y/N?')\n",
    "#     itr = 0\n",
    "    mdp_game()\n",
    "    list1=['N', 'Y']\n",
    "    b=rm.randint(0,1)\n",
    "    restart = list1[b]\n",
    "    print(\"RESTART SECTION APPROACHED\")\n",
    "    \n",
    "    if (itr[0] > 10):\n",
    "        \n",
    "        if (restart == 'N' or restart == 'n'):\n",
    "            print(\"GAME OVER\")\n",
    "            break\n",
    "        elif restart == 'Y' or restart == 'y':\n",
    "\n",
    "            \n",
    "             print(\"NEW GAME\")\n",
    "#             print(\"Itr #:\", itr)\n",
    "#             print(\"Total Iterations Completed\")\n",
    "\n",
    "            \n",
    "# print(\"StatesxActions Grid: \", grid)\n",
    "# print(\"reward:\", sum(reward))\n",
    "# print(\"reward Summed Array:\", (reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Working example of Q-learning---only problem is that it determines the Q-matrix based off of a pre-built reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = 2\n",
    "cols = 3\n",
    "grid = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "\n",
    "\n",
    "\n",
    "#Keeping track of how reward is calculated\n",
    "Lev = Levenshtein\n",
    "\n",
    "#For the reward, I have chosen a gradually increasing reward as the goal is approached with obstacles based on the Lev score\n",
    "reward_grid = [[0 for i in range(cols)] for i in range(rows)] #2States x 3 Actions\n",
    "reward = []\n",
    "prize = []\n",
    "\n",
    "print(\"Decision Modeling MDP Choice Game\")\n",
    "print(\"===========================================\")\n",
    "print(\"Follow the prompted Instructions\")\n",
    "    \n",
    "def mdp_game():\n",
    "    r = 0\n",
    "    \n",
    "    #Automate the choice for state\n",
    "    state = rm.randint(1,2)\n",
    "    if( state == 1 ):\n",
    "        print(\"You have selected Education\")\n",
    "        \n",
    "        print(\"\\nWhich action would you like to take?\")\n",
    "        print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "        #action = input(\"Which action would you like to accomplish to the candidate's application? \\n A) Accept\\n B) Decline\\n C) Review\\n Enter A or B or C:\")\n",
    "        \n",
    "        #Automate the choice for action\n",
    "        list1=['a', 'b', 'c']\n",
    "        b=rm.randint(0,2)\n",
    "        action = list1[b]\n",
    "        \n",
    "        if( action == ('A') or action == ('a')):\n",
    "            print(\"You have chosen to accept the candidate's application\")\n",
    "            print(\"Accept\")\n",
    "            \n",
    "            grid[0][0] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                r += 1\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r -= .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            reward_grid[0][0] += r\n",
    "                \n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            print( \"You have chosen to decline the candidate's application\" )\n",
    "            print(\"Decline\")\n",
    "           \n",
    "            grid[0][1] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                r -= 1\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r += .25\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            reward_grid[0][1] += r\n",
    "                \n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "            print(\"Continue\")\n",
    "            grid[0][2] += 1\n",
    "            mdp_game()\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 50):\n",
    "                r += .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r -= .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            reward_grid[0][2] += r \n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid choice!\")\n",
    "\n",
    "        \n",
    "    elif( state == 2 ):\n",
    "        print( \"You have selected Skills\" )\n",
    "        \n",
    "        print(\"\\nWhich action would you like to take?\")\n",
    "        print(\" A) Examine Another Section\\n B) Accept the Candidate's Application\\n C) Decline the Candidate's Application \\n\\nEnter A or B or C:\\n \")\n",
    "\n",
    "        #Automate the choice for action\n",
    "        list1=['a', 'b', 'c']\n",
    "        b=rm.randint(0,2)\n",
    "        action = list1[b]\n",
    "        \n",
    "        if( action == ('A') or action == ('a')):\n",
    "            print(\"You have chosen to accept the candidate's application\")\n",
    "            print(\"Accept\")\n",
    "            \n",
    "            grid[1][0] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 75):\n",
    "                r += 1\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r -= .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "                \n",
    "            reward_grid[1][0] += r \n",
    "                \n",
    "        elif( action == (\"B\") or action == (\"b\") ):\n",
    "            print( \"You have chosen to decline the candidate's application\" )\n",
    "            print(\"Decline\")\n",
    "            \n",
    "            grid[1][1] += 1\n",
    "            \n",
    "            if( Lev >= 75):\n",
    "                r -= 1\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r += .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            \n",
    "            reward_grid[1][1] += r \n",
    "                \n",
    "        elif( action == (\"C\") or action == (\"c\") ):\n",
    "            print( \"You have chosen to continue viewing the candidate's application\" )\n",
    "            print(\"Continue\")\n",
    "            grid[1][2] += 1\n",
    "            \n",
    "            #Keeping track of reward\n",
    "            if( Lev >= 50):\n",
    "                r += .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "            else:\n",
    "                r -= .33\n",
    "                reward.append(r)\n",
    "                reward[0] += 1\n",
    "\n",
    "            reward_grid[1][2] += r\n",
    "                \n",
    "            mdp_game()\n",
    "                \n",
    "        else:\n",
    "            print(\"Invalid choice!\")\n",
    "    else:\n",
    "        print(\"Invalid choice!\")   \n",
    "        \n",
    "\n",
    "while True:\n",
    "    \n",
    "        mdp_game()\n",
    "        \n",
    "        #Automate Restart of game\n",
    "        list1=['N', 'Y']\n",
    "        b=rm.randint(0,1)\n",
    "        restart = list1[b]\n",
    "    \n",
    "        if restart == 'N' or restart == 'n':\n",
    "            print(\"GAME OVER\")\n",
    "            break\n",
    "        elif restart == 'Y' or restart == 'y':\n",
    "            print(\"NEW GAME\")\n",
    "            #mdp_game()\n",
    "            \n",
    "print(\"StatesxActions Grid: \", grid)\n",
    "print(\"StatesxActions Reward: \", reward)\n",
    "# print(\"reward:\", sum(reward))\n",
    "\n",
    "\n",
    "GAMMA = 0.9 \n",
    "alpha = 1.\n",
    "\n",
    "Q = np.zeros(( 5 , 3 )) #initialize Q\n",
    "\n",
    "Re = np.array(reward_grid)\n",
    "\n",
    "def getMaxQ(state): \n",
    "    return max(Q[state,: ])\n",
    "\n",
    "def QLearning(state): \n",
    "    curAction = None \n",
    "    for action in range(2):\n",
    "        if (Re[state][action] ==- 1 ):\n",
    "            Q[state, action] = 0\n",
    "        else :\n",
    "            curAction = action\n",
    "            #\n",
    "            Q[state, action] = Q[state, action] + alpha * (Re[state][action] + GAMMA * getMaxQ(curAction)-Q[state, action])\n",
    "            \n",
    "\n",
    "count = 0     \n",
    "while count < 1000:\n",
    "    for i in range(2):\n",
    "        QLearning(i)\n",
    "    count += 1 \n",
    "print(Q/5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
