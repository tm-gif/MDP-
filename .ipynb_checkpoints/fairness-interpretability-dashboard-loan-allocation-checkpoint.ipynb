{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade fairlearn\n",
    "%pip install --upgrade interpret-community\n",
    "%pip install --upgrade raiwidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import GridSearch\n",
    "from fairlearn.reductions import DemographicParity, ErrorRate\n",
    "from fairlearn.datasets import fetch_adult\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "\n",
    "from sklearn import svm, neighbors, tree\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# SHAP Tabular Explainer\n",
    "from interpret.ext.blackbox import KernelExplainer\n",
    "from interpret.ext.blackbox import MimicExplainer\n",
    "from interpret.ext.glassbox import LGBMExplainableModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = fetch_adult(as_frame=True)\n",
    "X_raw, y = dataset['data'], dataset['target']\n",
    "X_raw[\"race\"].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_features = X_raw[['sex','race']]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, sensitive_features_train, sensitive_features_test = \\\n",
    "    train_test_split(X_raw, y, sensitive_features,\n",
    "                     test_size = 0.2, random_state=0, stratify=y)\n",
    "\n",
    "# Work around indexing bug\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "sensitive_features_train = sensitive_features_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "sensitive_features_test = sensitive_features_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, make_column_selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, make_column_selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LogisticRegression(solver=\"liblinear\", fit_intercept=True),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SHAP KernelExplainer\n",
    "# clf.steps[-1][1] returns the trained classification model\n",
    "explainer = MimicExplainer(model.steps[-1][1], \n",
    "                           X_train,\n",
    "                           LGBMExplainableModel,\n",
    "                           features=X_raw.columns, \n",
    "                           classes=['Rejected', 'Approved'],\n",
    "                           transformations=preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note we downsample the test data since visualization dashboard can't handle the full dataset\n",
    "global_explanation = explainer.explain_global(X_test[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_explanation.get_feature_importance_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can pass a specific data point or a group of data points to the explain_local function\n",
    "# E.g., Explain the first data point in the test set\n",
    "instance_num = 1\n",
    "local_explanation = explainer.explain_local(X_test[:instance_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction for the first member of the test set and explain why model made that prediction\n",
    "prediction_value = model.predict(X_test)[instance_num]\n",
    "\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()[prediction_value]\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()[prediction_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('local importance values: {}'.format(sorted_local_importance_values))\n",
    "print('local importance names: {}'.format(sorted_local_importance_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ExplanationDashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(global_explanation, model, dataset=X_test[:1000], true_y=y_test[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import FairnessDashboard\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "FairnessDashboard(sensitive_features=sensitive_features_test,\n",
    "                  y_true=y_test,\n",
    "                  y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairlearn is not yet fully compatible with Pipelines, so we have to pass the estimator only\n",
    "X_train_prep = preprocessor.transform(X_train).toarray()\n",
    "X_test_prep = preprocessor.transform(X_test).toarray()\n",
    "\n",
    "sweep = GridSearch(LogisticRegression(solver=\"liblinear\", fit_intercept=True),\n",
    "                   constraints=DemographicParity(),\n",
    "                   grid_size=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep.fit(X_train_prep, y_train,\n",
    "          sensitive_features=sensitive_features_train.sex)\n",
    "\n",
    "predictors = sweep.predictors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, disparities = [], []\n",
    "\n",
    "for predictor in predictors:\n",
    "    accuracy_metric_frame = MetricFrame(accuracy_score, y_train, predictor.predict(X_train_prep), sensitive_features=sensitive_features_train.sex)\n",
    "    selection_rate_metric_frame = MetricFrame(selection_rate, y_train, predictor.predict(X_train_prep), sensitive_features=sensitive_features_train.sex)\n",
    "    accuracies.append(accuracy_metric_frame.overall)\n",
    "    disparities.append(selection_rate_metric_frame.difference())\n",
    "    \n",
    "all_results = pd.DataFrame({\"predictor\": predictors, \"accuracy\": accuracies, \"disparity\": disparities})\n",
    "\n",
    "all_models_dict = {\"unmitigated\": model.steps[-1][1]}\n",
    "dominant_models_dict = {\"unmitigated\": model.steps[-1][1]}\n",
    "base_name_format = \"grid_{0}\"\n",
    "row_id = 0\n",
    "for row in all_results.itertuples():\n",
    "    model_name = base_name_format.format(row_id)\n",
    "    all_models_dict[model_name] = row.predictor\n",
    "    accuracy_for_lower_or_eq_disparity = all_results[\"accuracy\"][all_results[\"disparity\"] <= row.disparity]\n",
    "    if row.accuracy >= accuracy_for_lower_or_eq_disparity.max():\n",
    "        dominant_models_dict[model_name] = row.predictor\n",
    "    row_id = row_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from raiwidgets import FairnessDashboard\n",
    "\n",
    "dashboard_all = {}\n",
    "for name, predictor in all_models_dict.items():\n",
    "    value = predictor.predict(X_test_prep)\n",
    "    dashboard_all[name] = value\n",
    "    \n",
    "dominant_all = {}\n",
    "for name, predictor in dominant_models_dict.items():\n",
    "    dominant_all[name] = predictor.predict(X_test_prep)\n",
    "\n",
    "FairnessDashboard(sensitive_features=sensitive_features_test, \n",
    "                  y_true=y_test,\n",
    "                  y_pred=dominant_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
